#!/usr/bin/env python3
"""
Mooer Camera NG - ä¸»å…¥å£
é‡æ„åçš„æ¨¡å—åŒ–æ™ºèƒ½è§†è§’æ§åˆ¶ç³»ç»Ÿ
"""

import faulthandler
import fcntl
import os
import subprocess
import sys
import time
from collections import deque

# å¯ç”¨ faulthandlerï¼Œåœ¨å´©æºƒæ—¶æ‰“å° Python å †æ ˆ
faulthandler.enable()

from camera_ng import (
    DEFAULT_NUM_STEPS, DEFAULT_TOTAL_ANGLE,
    ROTATION_SPEED, TRACK_CHECK_INTERVAL, DETECTION_INTERVAL,
    PTZ_SPEED, PTZ_FAST_SPEED,
    TRACKER_MAX_AGE, TRACKER_MIN_HITS,
    CAPTURE_WIDTH, CAPTURE_HEIGHT, LOCK_FILE,
    CAMERA_RTSP, CAMERA_RTSP_SUB, STREAM_LOW_LATENCY, DEVICE_SERIAL, ACCESS_TOKEN,
    HAND_SIDE_MODE,
    CameraController, VisionAnalyzer, HandRaiseDetector, XiaoxiaoTTS, AsyncVoiceQueue,
    PersonTracker, TrackingMemory
)
from camera_ng.handlers import (
    HandGestureHandler, HandGesture,
    RecordingManager, SmartShotWorker,
)

TELEGRAM_TARGET = "1115213761"


def run_openclaw_send(cmd: list[str], retries: int = 2, timeout_sec: int = 20) -> bool:
    """å‘é€æ¶ˆæ¯ï¼ˆå«è¶…æ—¶ä¸é‡è¯•ï¼‰ï¼Œé¿å…åå°ä»»åŠ¡é•¿æœŸå ç”¨"""
    for attempt in range(1, retries + 1):
        try:
            subprocess.run(cmd, check=True, timeout=timeout_sec)
            return True
        except Exception as e:
            if attempt == retries:
                print(f"âŒ OpenClaw å‘é€å¤±è´¥: {e}")
                return False
            print(f"âš ï¸ OpenClaw å‘é€å¤±è´¥ï¼Œé‡è¯• {attempt}/{retries - 1}: {e}")
            time.sleep(0.6)
    return False


def validate_config():
    """éªŒè¯å¿…è¦é…ç½®æ˜¯å¦å­˜åœ¨"""
    errors = []
    
    if CAMERA_RTSP is None:
        errors.append("  - camera.rtsp_url: RTSP æµåœ°å€æœªé…ç½®")
    if DEVICE_SERIAL is None:
        errors.append("  - camera.device_serial: è®¾å¤‡åºåˆ—å·æœªé…ç½®")
    if ACCESS_TOKEN is None:
        errors.append("  - camera.access_token: è¤çŸ³äº‘ AccessToken æœªé…ç½®")
    
    if errors:
        print("\n" + "=" * 60)
        print("âŒ é…ç½®é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„æ•æ„Ÿä¿¡æ¯")
        print("=" * 60)
        print("\nè¯·åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€åˆ›å»º camera-config.yaml æ–‡ä»¶ï¼š")
        print("  1. ./memory/camera-config.yaml")
        print("  2. ~/.openclaw/workspace/memory/camera-config.yaml")
        print("  3. ~/clawd/memory/camera-config.yaml")
        print("  4. ~/.config/mooer-camera/config.yaml")
        print("\nç¼ºå¤±çš„é…ç½®é¡¹ï¼š")
        for err in errors:
            print(err)
        print("\ncamera-config.yaml ç¤ºä¾‹æ ¼å¼ï¼š")
        print("""
camera:
  rtsp_url: "rtsp://admin:PASSWORD@IP:554/h264/ch1/main/av_stream"
  device_serial: "YOUR_DEVICE_SERIAL"
  access_token: "YOUR_ACCESS_TOKEN"
  capture:
    seek_time: "00:00:00.5"
    resolution:
      width: 640
      height: 360
    quality: 2
  ptz:
    rotation_speed: 28
""")
        print("=" * 60)
        sys.exit(1)


class SmartCamera:
    """æ™ºèƒ½æ‘„åƒå¤´ç³»ç»Ÿï¼ˆhuman ä¸“ç”¨ï¼‰"""

    def __init__(self):
        self.camera = CameraController()
        self.vision = VisionAnalyzer()

    def human(self, num_steps: int = DEFAULT_NUM_STEPS,
              total_angle: float = DEFAULT_TOTAL_ANGLE,
              use_gpu: bool = False, smart: bool = True,
              fast: bool = False, keep_stream: bool = True,
              center_and_wait: bool = False) -> bool:
        """æ‰§è¡Œ human æ‰«æï¼Œæ‰¾åˆ°äººè¿”å›True"""
        if not self.camera.stream_active:
            print("ğŸ¥ å¯åŠ¨è§†é¢‘æµ...")
            if not self.camera.start_stream(use_gpu=use_gpu):
                print("âŒ æ— æ³•å¯åŠ¨è§†é¢‘æµ")
                return False
            time.sleep(0.5)
        else:
            print("ğŸ”„ å¤ç”¨å·²æœ‰è§†é¢‘æµ...")

        try:
            self.camera.center_and_wait_mode = center_and_wait
            
            if smart:
                result = self.camera.human_steps_smart(self.vision)
            elif fast:
                result = self.camera.human_steps_fast(self.vision)
            else:
                result = self.camera.human_steps(
                    self.vision, num_steps=num_steps, total_angle=total_angle
                )
        finally:
            if not keep_stream:
                self.camera.stop_stream()

        return result
    
    def stop(self):
        """å®Œå…¨åœæ­¢å¹¶æ¸…ç†èµ„æº"""
        self.camera.stop_stream()
        print("âœ… æ™ºèƒ½æ‘„åƒå¤´ç³»ç»Ÿå·²åœæ­¢")
    
    def human_smart_only(self) -> bool:
        """æ™ºèƒ½æ‰«æï¼ˆå¤ç”¨å·²æœ‰æµï¼‰"""
        return self.human(smart=True)


class SingleInstanceLock:
    """å•å®ä¾‹æ–‡ä»¶é”"""

    def __init__(self, lock_file: str = LOCK_FILE):
        self.lock_file = lock_file
        self.fd = None

    def acquire(self) -> bool:
        """è·å–æ–‡ä»¶é”"""
        try:
            self.fd = open(self.lock_file, "w")
            fcntl.flock(self.fd.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            self.fd.write(str(os.getpid()))
            self.fd.flush()
            return True
        except (IOError, OSError):
            if self.fd:
                self.fd.close()
                self.fd = None
            return False

    def release(self):
        """é‡Šæ”¾æ–‡ä»¶é”"""
        if self.fd:
            try:
                fcntl.flock(self.fd.fileno(), fcntl.LOCK_UN)
                self.fd.close()
                if os.path.exists(self.lock_file):
                    os.remove(self.lock_file)
            except (IOError, OSError):
                pass
            finally:
                self.fd = None

    def __enter__(self):
        if not self.acquire():
            raise RuntimeError("å¦ä¸€ä¸ªå®ä¾‹æ­£åœ¨è¿è¡Œ")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()


def capture_and_send_current_view(
    camera: CameraController,
    message: str,
    send_to_tg: bool = False,
) -> bool:
    """åŸºäºå½“å‰ç”»é¢æŠ“æ‹ï¼›å¯é€‰å‘é€ Telegram"""
    output_dir = os.path.expanduser("~/Desktop/capture/pictures")
    os.makedirs(output_dir, exist_ok=True)
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    ms = int((time.time() % 1) * 1000)
    img_path = os.path.join(output_dir, f"{timestamp}_{ms:03d}.jpg")

    camera.capture(output_path=img_path, full_quality=True)
    print(f"ğŸ“¸ å·²ä¿å­˜é«˜è´¨é‡æŠ“æ‹: {img_path}")

    if not send_to_tg:
        print("ğŸ“­ Telegram å‘é€å·²å…³é—­ï¼ˆä½¿ç”¨ --tg å¼€å¯ï¼‰")
        return True

    try:
        send_cmd = [
            "openclaw", "message", "send",
            "--channel", "telegram",
            "--target", TELEGRAM_TARGET,
            "--media", img_path,
            "--message", message
        ]
        print("ğŸ“¤ æ­£åœ¨é€šè¿‡ OpenClaw å‘é€ç…§ç‰‡...")
        ok = run_openclaw_send(send_cmd)
        if ok:
            print("âœ… ç…§ç‰‡å‘é€æˆåŠŸï¼")
        return ok
    except Exception as e:
        print(f"âŒ ç…§ç‰‡å‘é€å¤±è´¥: {e}")
        return False


def send_greeting_voice(tts: XiaoxiaoTTS, message: str, send_to_tg: bool = False) -> bool:
    """å‘é€ä¸­æ–‡é—®å€™è¯­éŸ³ï¼ˆå¯é€‰ Telegramï¼‰"""
    try:
        if not send_to_tg:
            print("ğŸ“­ Telegram è¯­éŸ³å‘é€å·²å…³é—­ï¼ˆä½¿ç”¨ --tg å¼€å¯ï¼‰")
            return True

        voice_path = tts.synthesize(message)
        send_cmd = [
            "openclaw", "message", "send",
            "--channel", "telegram",
            "--target", TELEGRAM_TARGET,
            "--media", voice_path,
            "--message", "å³æ‰‹æ‰‹åŠ¿è¯­éŸ³é—®å€™",
        ]
        print("ğŸ”Š æ­£åœ¨å‘é€é—®å€™è¯­éŸ³...")
        ok = run_openclaw_send(send_cmd)
        if ok:
            print("âœ… è¯­éŸ³å‘é€æˆåŠŸï¼")
        return ok
    except Exception as e:
        print(f"âŒ è¯­éŸ³å‘é€å¤±è´¥: {e}")
        return False


# å·²è¿ç§»åˆ° handlers.py


# å·²è¿ç§»åˆ° handlers.py


def handle_gesture_event(
    event,
    recording_mgr: RecordingManager | None,
    smart_shot_worker: SmartShotWorker,
    current_time: float,
) -> bool:
    """å¤„ç†æ‰‹åŠ¿äº‹ä»¶ï¼Œè¿”å›æ˜¯å¦æˆåŠŸå¤„ç†"""
    from camera_ng.handlers import HandGesture

    if event.gesture == HandGesture.LEFT_HAND:
        # å·¦æ‰‹æ§åˆ¶å½•åƒå¼€å…³
        print(f"\n   âœ‹ æ£€æµ‹åˆ°å·¦æ‰‹æŠ¬èµ·ï¼Œåˆ‡æ¢å½•åƒçŠ¶æ€... ({event.reason})")
        if recording_mgr is not None:
            recording_mgr.toggle(current_time)
            return True
        return False

    elif event.gesture == HandGesture.RIGHT_HAND:
        # å³æ‰‹è§¦å‘ Smart-Shot
        print(f"\n   ğŸ™‹ æ£€æµ‹åˆ°å³æ‰‹æŠ¬èµ·ï¼Œè§¦å‘ Smart-Shot... ({event.reason})")
        return smart_shot_worker.submit("å³æ‰‹", event.reason)

    return False


def check_single_instance():
    """æ£€æŸ¥æ˜¯å¦å•å®ä¾‹è¿è¡Œ"""
    lock = SingleInstanceLock()
    if not lock.acquire():
        print("âŒ é”™è¯¯ï¼šå¦ä¸€ä¸ª mooer_camera å®ä¾‹æ­£åœ¨è¿è¡Œ")
        print(f"   é”æ–‡ä»¶: {LOCK_FILE}")
        print("   è¯·å…ˆåœæ­¢å…¶ä»–å®ä¾‹å†è¿è¡Œ")
        sys.exit(1)
    return lock


def track_human_realtime(num_steps: int = DEFAULT_NUM_STEPS,
                         total_angle: float = 360,
                         detection_interval: int = DETECTION_INTERVAL,
                         use_gpu: bool = False,
                         tracking_mode: str = "hybrid",
                         smart_shot: bool = False,
                         quick_mode: bool = False,
                         send_to_tg: bool = False,
                         enable_miss: bool = False,
                         voice_drop_oldest: bool = False,
                         mute_voice: bool = False,
                         low_latency_stream: bool = STREAM_LOW_LATENCY,
                         ptz_speed: int = PTZ_SPEED,
                         ptz_speed_fast: int = PTZ_FAST_SPEED) -> None:
    """å®æ—¶ç›®æ ‡è·Ÿè¸ªæ¨¡å¼ - ä½¿ç”¨é‡æ„åçš„å¤„ç†å™¨"""
    cam = SmartCamera()
    cam.camera.ptz_speed_default = int(max(1, min(7, ptz_speed)))
    effective_detection_interval = 1 if quick_mode else detection_interval
    tracker = PersonTracker(
        yolo_model="yolov8n",
        confidence=0.5,
        detection_interval=effective_detection_interval
    )

    # çŠ¶æ€ç®¡ç†
    cycle_count = 0
    tracking_mode = (tracking_mode or "hybrid").lower()
    software_assist_active = tracking_mode == "software"
    person_found = False
    analyzing = False
    lost_count = 0
    LOST_THRESHOLD = 5

    fps_history = deque(maxlen=30)
    last_time = time.time()
    offset_history_size = 3 if smart_shot else 5
    offset_x_history = deque(maxlen=offset_history_size)
    offset_y_history = deque(maxlen=offset_history_size)
    recenter_candidate_count = 0
    last_recenter_time = 0.0

    # äº‘å°æ§åˆ¶å‚æ•°
    RECENTER_CONFIRM_FRAMES = 3
    RECENTER_COOLDOWN = 1.4 if smart_shot else 1.2
    BASE_RECENTER_X_THRESHOLD = 0.5
    BASE_RECENTER_Y_THRESHOLD_UP = 0.28
    BASE_RECENTER_Y_THRESHOLD_DOWN = 0.32
    POST_FOUND_REVERSE_STEP_SEC = 0.15
    POST_FOUND_REVERSE_MIN_OFFSET = 0.08
    STABLE_X_DEADBAND = 0.12
    STABLE_Y_DEADBAND = 0.16
    STABLE_HOLD_SEC = 0.45
    STABLE_VEL_THRESHOLD = 0.03
    CALIB_DONE_MIN_INTERVAL_SEC = 2.0
    MICRO_STEP_MAX_SEC = 0.09
    MICRO_STEP_MIN_SEC = 0.02
    MICRO_STEP_Y_THRESHOLD_UP = 0.10
    MICRO_STEP_Y_THRESHOLD_DOWN = 0.12
    FAST_TARGET_VEL_X = 0.06
    FAST_TURN_BOOST = 1.25 if smart_shot else 1.6
    FAST_TURN_MAX_SEC = 0.09 if smart_shot else 0.12
    # å‚ç›´äº‘å°å¯ç”¨èŒƒå›´è¾ƒå°ï¼ˆçº¦ 20Â°ï¼‰ï¼ŒSmart-Shot å‚ç›´çº åè¦æ›´ä¿å®ˆ
    SMART_SHOT_FOOT_MARGIN_MIN = 0.05
    SMART_SHOT_HEAD_MARGIN_MIN = 0.08
    SMART_SHOT_MARGIN_GAIN = 1.8
    SMART_SHOT_MARGIN_PUSH_MAX = 0.20
    SMART_SHOT_FULL_BODY_HEIGHT_MAX = 0.78
    SMART_SHOT_HEAD_LOW_TRIGGER = 0.58
    SMART_SHOT_HEAD_LOW_PUSH_GAIN = 1.2
    SMART_SHOT_HEAD_LOW_PUSH_MAX = 0.22

    calibrating_active = False
    stable_candidate_since = 0.0
    last_calibration_done_time = 0.0

    status_voice_last_ts: dict[str, float] = {}

    def broadcast_status(text: str, min_interval_sec: float = 0.0) -> None:
        if mute_voice:
            return
        now = time.time()
        last_ts = status_voice_last_ts.get(text, 0.0)
        if min_interval_sec > 0 and (now - last_ts) < min_interval_sec:
            return
        status_voice_last_ts[text] = now
        if voice_queue is not None:
            voice_queue.enqueue(text)

    def apply_micro_recenter(offset_x: float, offset_y: float, speed_x: float = 0.0) -> bool:
        moved = False
        abs_x = abs(offset_x)
        fast_move = speed_x >= FAST_TARGET_VEL_X and abs_x >= 0.10
        cmd_speed = int(max(1, min(7, ptz_speed_fast if fast_move else ptz_speed)))
        if abs_x >= 0.06:
            step_x = min(MICRO_STEP_MAX_SEC, max(MICRO_STEP_MIN_SEC, abs_x * 0.06))
            if fast_move:
                step_x = min(FAST_TURN_MAX_SEC, step_x * FAST_TURN_BOOST)
            direction_x = "left" if offset_x < 0 else "right"
            if cam.camera.ptz_turn(direction_x, step_x, speed=cmd_speed):
                moved = True

        abs_y = abs(offset_y)
        y_threshold = MICRO_STEP_Y_THRESHOLD_UP if offset_y < 0 else MICRO_STEP_Y_THRESHOLD_DOWN
        if abs_y >= y_threshold:
            step_y_scale = 0.06 if offset_y < 0 else 0.04
            step_y = min(0.06, max(0.02, abs_y * step_y_scale))
            direction_y = "up" if offset_y < 0 else "down"
            if cam.camera.ptz_turn(direction_y, step_y, speed=cmd_speed):
                moved = True

        return moved

    # Smart-Shot ç»„ä»¶
    tts = XiaoxiaoTTS() if not mute_voice else None
    voice_queue = None
    if tts is not None:
        voice_queue = AsyncVoiceQueue(tts=tts, max_queue_size=8, drop_oldest=voice_drop_oldest)
        voice_queue.start()
    hand_detector = HandRaiseDetector(
        infer_imgsz=224 if quick_mode else 256,
        hand_side_mode=HAND_SIDE_MODE,
    ) if smart_shot else None
    gesture_handler = (
        HandGestureHandler(
            detector=hand_detector,
            confirm_frames=1 if quick_mode else 2,
            release_frames=2 if quick_mode else 3,
            cooldown_sec=0.6 if quick_mode else 1.0,
            log_interval_sec=0.5 if quick_mode else 1.0,
            detect_interval_sec=0.25 if quick_mode else 0.55,
        )
        if smart_shot and hand_detector else None
    )
    recording_mgr = (
        RecordingManager(
            rtsp_url=CAMERA_RTSP,
            tts=tts,
            voice_enqueue=(voice_queue.enqueue if voice_queue is not None else None),
            toggle_cooldown_sec=1.5,
            auto_start_on_person_found=False,
        )
        if smart_shot else None
    )
    def smart_shot_task_callback(camera, hand_text, hand_reason, tts_instance):
        """Smart-Shot ä»»åŠ¡å›è°ƒ"""
        capture_and_send_current_view(
            camera,
            f"Albertï¼Œæˆ‘æ£€æµ‹åˆ°ä½ æŠ¬{hand_text}ï¼Œå·²ä¸ºä½ æŠ“æ‹ï¼ğŸ“¸",
            send_to_tg=send_to_tg,
        )

    smart_shot_worker = (
        SmartShotWorker(
            camera=cam.camera,
            tts=tts,
            telegram_target=TELEGRAM_TARGET,
            voice_enqueue=(voice_queue.enqueue if voice_queue is not None else None),
            max_queue_size=3,
            task_callback=smart_shot_task_callback,
        )
        if smart_shot else None
    )

    if smart_shot and smart_shot_worker:
        smart_shot_worker.start()

    frame_sleep = 0.005 if quick_mode else 0.03
    recenter_pause = 0.2 if quick_mode else 0.5

    print("\n" + "=" * 60)
    print("ğŸ” å¯åŠ¨å®æ—¶ç›®æ ‡è·Ÿè¸ªæ¨¡å¼ (Real-time + SORT)")
    print("=" * 60)
    print(f"é…ç½®: {num_steps}æ­¥/{total_angle}Â°")
    print(f"YOLOæ£€æµ‹é—´éš”: æ¯{effective_detection_interval}å¸§")
    print(f"è·Ÿè¸ªå™¨: SORT (max_age={TRACKER_MAX_AGE}, min_hits={TRACKER_MIN_HITS})")
    print(f"è§†é¢‘è§£ç : {'GPU (CUDA)' if use_gpu else 'CPU'}")
    print(f"ğŸ¯ è·Ÿè¸ªæ¨¡å¼: {tracking_mode}{' (software-assist)' if software_assist_active else ''}")
    print(f"ğŸ›ï¸ PTZé€Ÿåº¦: å¸¸è§„{int(max(1, min(7, ptz_speed)))} / è¿½èµ¶{int(max(1, min(7, ptz_speed_fast)))}")
    print(f"ğŸ“º è·Ÿè¸ªæµ: {'å­ç æµ' if CAMERA_RTSP_SUB else 'ä¸»ç æµ'} | {'ä½å»¶è¿Ÿ' if low_latency_stream else 'å¸¸è§„å»¶è¿Ÿ'}")
    if smart_shot:
        print("ğŸ“¸ Smart-Shot: å³æ‰‹å¼‚æ­¥ä¿å­˜é«˜è´¨é‡æŠ“æ‹ï¼Œå·¦æ‰‹æŠ¬èµ·å¼€å§‹/åœæ­¢å½•åƒ")
        print("ğŸ–¼ï¸ æŠ“æ‹ç›®å½•: ~/Desktop/capture/pictures")
        print(f"ğŸ“¨ Telegram å‘é€: {'å¼€å¯' if send_to_tg else 'å…³é—­ï¼ˆé»˜è®¤ï¼‰'}")
        if hand_detector is None or hand_detector.model is None:
            print("âš ï¸ Smart-Shot pose æ¨¡å‹ä¸å¯ç”¨ï¼ŒæŠ¬æ‰‹æ£€æµ‹ä¸ä¼šè§¦å‘")
        if tts is None or not tts.is_available():
            print("âš ï¸ æ™“æ™“ TTS ä¸å¯ç”¨ï¼Œå³æ‰‹æŠ¬èµ·åä¸ä¼šå‘é€è¯­éŸ³")
        print("ğŸ“¬ Smart-Shot é˜Ÿåˆ—ç­–ç•¥: drop_oldestï¼ˆé˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§ä»»åŠ¡ï¼‰")
        print(f"ğŸ™‹ æ‰‹åŠ¿æ£€æµ‹é¢‘ç‡: æ¯ {0.20 if quick_mode else 0.40:.2f}s ä¸€æ¬¡ï¼ˆä¼˜å…ˆè·Ÿéšæµç•…åº¦ï¼‰")
        print(f"ğŸ™‹ æ‰‹åŠ¿æ¨¡å‹è¾“å…¥: {224 if quick_mode else 256}px")
        print(f"ğŸ™‹ å·¦å³æ‰‹æ¨¡å¼: {HAND_SIDE_MODE}ï¼ˆauto/normal/swappedï¼‰")
        print("ğŸ¬ å½•åƒç­–ç•¥: ä»…å·¦æ‰‹æŠ¬èµ·å¼€å§‹ï¼Œä¸¢å¤±ç›®æ ‡æ—¶è‡ªåŠ¨åœæ­¢")
    print(f"ğŸ”” ç›®æ ‡ä¸¢å¤±æ’­æŠ¥: {'å¼€å¯' if enable_miss else 'å…³é—­ï¼ˆä½¿ç”¨ --enable-miss å¼€å¯ï¼‰'}")
    if mute_voice:
        print("ğŸ”‡ æœ¬æœºè¯­éŸ³æ’­æŠ¥: å…³é—­ï¼ˆ-m/--muteï¼‰")
    else:
        print(f"ğŸ”Š è¯­éŸ³é˜Ÿåˆ—: {'drop_oldest' if voice_drop_oldest else 'keep_all'}")
    if quick_mode:
        print("âš¡ Quick æ¨¡å¼: é«˜é¢‘æ£€æµ‹ + æ›´ä½å†·é™æ—¶é—´")
    if tracking_mode in ("native", "hybrid"):
        print("â„¹ï¸ native/hybrid ä¾èµ–æ‘„åƒå¤´ç«¯å·²å¼€å¯äººåƒè¿½è¸ªï¼›å¤±æ•ˆæ—¶ hybrid ä¼šè‡ªåŠ¨åˆ‡å›è½¯ä»¶æ‰«æ")
    print("æŒ‰ Ctrl+C åœæ­¢è¿½è¸ª")
    print("=" * 60 + "\n")

    tracking_rtsp = CAMERA_RTSP_SUB or CAMERA_RTSP
    if not cam.camera.start_stream(use_gpu=use_gpu, rtsp_url=tracking_rtsp, low_latency=low_latency_stream):
        print("âŒ æ— æ³•å¯åŠ¨è§†é¢‘æµ")
        return

    try:
        while True:
            cycle_count += 1
            current_time = time.time()

            fps_history.append(1.0 / (current_time - last_time + 0.001))
            avg_fps = sum(fps_history) / len(fps_history)
            last_time = current_time

            if not person_found:
                if tracking_mode in ("native", "hybrid") and not software_assist_active:
                    frame = cam.camera.get_frame()
                    if frame is None:
                        time.sleep(frame_sleep)
                        continue

                    tracks = tracker.update(frame)
                    main_person = tracker.get_main_person()
                    if main_person is not None and tracks:
                        person_found = True
                        analyzing = True
                        lost_count = 0
                        recenter_candidate_count = 0
                        calibrating_active = False
                        stable_candidate_since = 0.0
                        print("âœ… native è·Ÿè¸ªæ£€æµ‹åˆ°ç›®æ ‡")
                        broadcast_status("ç›®æ ‡æ•è·", min_interval_sec=0.8)
                        cam.camera.tracking_memory.reset()
                        if recording_mgr:
                            recording_mgr.on_person_found()
                        time.sleep(frame_sleep)
                        continue

                    lost_count += 1
                    if tracking_mode == "hybrid" and lost_count >= LOST_THRESHOLD:
                        print("âš ï¸ native è·Ÿè¸ªæœªç¨³å®šæ•è·ï¼Œåˆ‡æ¢åˆ°è½¯ä»¶æ‰«æå…œåº•...")
                        software_assist_active = True
                        lost_count = 0
                    time.sleep(frame_sleep)
                    continue

                # æ‰«ææ‰¾äºº
                print(f"\n{'=' * 60}")
                print(f"ğŸ”„ ç¬¬ {cycle_count} è½® | æ‰§è¡Œæ™ºèƒ½æ‰«æ...")
                print(f"{'=' * 60}")

                person_found = cam.human_smart_only()

                if person_found:
                    print("âœ… æ‰¾åˆ°ç›®æ ‡ï¼")
                    if software_assist_active:
                        print("ğŸ§© è½¯ä»¶æ‰«æå…œåº•æˆåŠŸï¼Œå›äº¤ native è·Ÿè¸ª")
                    broadcast_status("ç›®æ ‡æ•è·", min_interval_sec=0.8)
                    cam.camera.tracking_memory.reset()

                    if not cam.camera.stream_active:
                        if not cam.camera.start_stream(
                            use_gpu=use_gpu,
                            rtsp_url=tracking_rtsp,
                            low_latency=low_latency_stream,
                        ):
                            return
                        time.sleep(0.5)

                    # åˆå§‹åŒ–è·Ÿè¸ª
                    init_attempts = 0
                    max_init_attempts = 10

                    while init_attempts < max_init_attempts:
                        frame = cam.camera.get_frame()
                        if frame is not None:
                            tracks = tracker.update(frame, force_detect=True)
                            if tracks:
                                analyzing = True
                                lost_count = 0
                                break
                        init_attempts += 1
                        time.sleep(0.1)
                    else:
                        person_found = False
                        continue

                    analyzing = True
                    lost_count = 0
                    if tracking_mode == "hybrid":
                        software_assist_active = False
                    calibrating_active = False
                    stable_candidate_since = 0.0

                    # æ‰¾åˆ°ç›®æ ‡åï¼ŒæŒ‰äººç‰©åç§»åšä¸€æ¬¡åæ–¹å‘å¾®è°ƒï¼ˆæ›¿ä»£å›ºå®šç­‰å¾…ï¼‰
                    init_main = tracker.get_main_person()
                    if init_main is not None and (tracking_mode == "software" or software_assist_active):
                        init_cx = (init_main.bbox[0] + init_main.bbox[2]) / 2
                        init_offset_x = (init_cx - CAPTURE_WIDTH / 2) / (CAPTURE_WIDTH / 2)
                        if init_offset_x >= POST_FOUND_REVERSE_MIN_OFFSET:
                            cam.camera.ptz_turn("left", POST_FOUND_REVERSE_STEP_SEC)
                            print(f"â†©ï¸ æ•è·ååå‘å¾®è°ƒ: left {POST_FOUND_REVERSE_STEP_SEC:.2f}s")
                        elif init_offset_x <= -POST_FOUND_REVERSE_MIN_OFFSET:
                            cam.camera.ptz_turn("right", POST_FOUND_REVERSE_STEP_SEC)
                            print(f"â†ªï¸ æ•è·ååå‘å¾®è°ƒ: right {POST_FOUND_REVERSE_STEP_SEC:.2f}s")

                    # æ‰¾åˆ°ç›®æ ‡ï¼šæŒ‰é…ç½®å†³å®šæ˜¯å¦è‡ªåŠ¨å¼€å§‹å½•åƒ
                    if recording_mgr:
                        recording_mgr.on_person_found()
                else:
                    print("æœªæ‰¾åˆ°ï¼Œç»§ç»­æ‰«æ...")
                    if not cam.camera.start_stream(
                        use_gpu=use_gpu,
                        rtsp_url=tracking_rtsp,
                        low_latency=low_latency_stream,
                    ):
                        cam.camera.start_stream(
                            use_gpu=use_gpu,
                            rtsp_url=tracking_rtsp,
                            low_latency=low_latency_stream,
                        )
                    time.sleep(0.5)

            elif analyzing:
                # å®æ—¶è·Ÿè¸ªæ¨¡å¼
                frame = cam.camera.get_frame()
                if frame is None:
                    time.sleep(0.001)
                    continue

                tracks = tracker.update(frame)
                main_person = tracker.get_main_person()

                if main_person:
                    # è®¡ç®—åç§»å¹¶å¹³æ»‘
                    cx = (main_person.bbox[0] + main_person.bbox[2]) / 2
                    cy = (main_person.bbox[1] + main_person.bbox[3]) / 2
                    offset_x = (cx - CAPTURE_WIDTH/2) / (CAPTURE_WIDTH/2)
                    offset_y = (cy - CAPTURE_HEIGHT/2) / (CAPTURE_HEIGHT/2)

                    offset_x_history.append(offset_x)
                    offset_y_history.append(offset_y)
                    smoothed_offset_x = sum(offset_x_history) / len(offset_x_history)
                    smoothed_offset_y = sum(offset_y_history) / len(offset_y_history)
                    effective_offset_y = smoothed_offset_y

                    if smart_shot:
                        top_margin_ratio = max(0.0, float(main_person.bbox[1]) / CAPTURE_HEIGHT)
                        bottom_margin_ratio = max(0.0, float(CAPTURE_HEIGHT - main_person.bbox[3]) / CAPTURE_HEIGHT)
                        person_height_ratio = max(0.0, float(main_person.bbox[3] - main_person.bbox[1]) / CAPTURE_HEIGHT)

                        # äººåƒè¿‡é«˜é€šå¸¸å—è§†è§’/FOVé™åˆ¶ï¼Œç»§ç»­å‹å‚ç›´äº‘å°æ”¶ç›Šæœ‰é™ä¸”æ›´å®¹æ˜“æ‰“åˆ°ä¸Šä¸‹é™ä½
                        can_push_for_foot = person_height_ratio <= SMART_SHOT_FULL_BODY_HEIGHT_MAX

                        if can_push_for_foot and bottom_margin_ratio < SMART_SHOT_FOOT_MARGIN_MIN:
                            push_down = (SMART_SHOT_FOOT_MARGIN_MIN - bottom_margin_ratio) * SMART_SHOT_MARGIN_GAIN
                            push_down = min(SMART_SHOT_MARGIN_PUSH_MAX, push_down)
                            effective_offset_y = max(effective_offset_y, push_down)

                        if top_margin_ratio < SMART_SHOT_HEAD_MARGIN_MIN:
                            push_up = (SMART_SHOT_HEAD_MARGIN_MIN - top_margin_ratio) * SMART_SHOT_MARGIN_GAIN
                            push_up = min(SMART_SHOT_MARGIN_PUSH_MAX, push_up)
                            effective_offset_y = min(effective_offset_y, -push_up)

                        # å¤´éƒ¨æ˜æ˜¾è½åœ¨ç”»é¢ä¸‹åŠåŒºæ—¶ï¼Œä¼˜å…ˆè§¦å‘å‘ä¸‹ä¿®æ­£ï¼ˆè¡¥å¿â€œå¤´åœ¨åº•éƒ¨ä¸è·Ÿéšâ€ï¼‰
                        if top_margin_ratio > SMART_SHOT_HEAD_LOW_TRIGGER:
                            head_low_push = (top_margin_ratio - SMART_SHOT_HEAD_LOW_TRIGGER) * SMART_SHOT_HEAD_LOW_PUSH_GAIN
                            head_low_push = min(SMART_SHOT_HEAD_LOW_PUSH_MAX, head_low_push)
                            effective_offset_y = max(effective_offset_y, head_low_push)

                    ptz_control_active = (tracking_mode == "software") or software_assist_active

                    # æ›´æ–°è¿åŠ¨è®°å¿†
                    current_angle = cam.camera.tracking_memory.last_angle
                    if smoothed_offset_x < -0.3:
                        current_angle = (current_angle - 20) % 360
                    elif smoothed_offset_x > 0.3:
                        current_angle = (current_angle + 20) % 360
                    cam.camera.tracking_memory.update(current_angle)

                    # å±…ä¸­é€»è¾‘
                    person_width_ratio = (main_person.bbox[2] - main_person.bbox[0]) / CAPTURE_WIDTH
                    dynamic_x_threshold = BASE_RECENTER_X_THRESHOLD + min(0.25, person_width_ratio * 0.35)
                    dynamic_y_threshold = (
                        BASE_RECENTER_Y_THRESHOLD_UP
                        if effective_offset_y < 0
                        else BASE_RECENTER_Y_THRESHOLD_DOWN
                    )
                    need_recenter = (
                        abs(smoothed_offset_x) > dynamic_x_threshold
                        or abs(effective_offset_y) > dynamic_y_threshold
                    )

                    if need_recenter:
                        recenter_candidate_count += 1
                    else:
                        recenter_candidate_count = 0

                    if (
                        ptz_control_active
                        and
                        recenter_candidate_count >= RECENTER_CONFIRM_FRAMES
                        and (current_time - last_recenter_time) >= RECENTER_COOLDOWN
                    ):
                        if not calibrating_active:
                            broadcast_status("æ ¡å‡†ä¸­", min_interval_sec=0.5)
                        calibrating_active = True
                        stable_candidate_since = 0.0
                        speed_x = 0.0
                        if len(offset_x_history) >= 2:
                            speed_x = abs(offset_x_history[-1] - offset_x_history[-2])
                        print(f"\n   ğŸ¯ æŒç»­åç§»è§¦å‘å¾®æ­¥è·Ÿéš: æ°´å¹³{smoothed_offset_x:+.2f}, å‚ç›´{effective_offset_y:+.2f}")
                        apply_micro_recenter(smoothed_offset_x, effective_offset_y, speed_x)
                        recenter_candidate_count = 0
                        last_recenter_time = current_time
                        time.sleep(frame_sleep)

                    # æ ¡å‡†å®Œæˆåˆ¤å®šï¼šè¿›å…¥ä¸­å¿ƒæ­»åŒºå¹¶æŒç»­ç¨³å®šä¸€æ®µæ—¶é—´
                    if ptz_control_active and calibrating_active:
                        in_deadband = (
                            abs(smoothed_offset_x) <= STABLE_X_DEADBAND
                            and abs(smoothed_offset_y) <= STABLE_Y_DEADBAND
                        )
                        if len(offset_x_history) >= 2 and len(offset_y_history) >= 2:
                            vel_x = abs(offset_x_history[-1] - offset_x_history[-2])
                            vel_y = abs(offset_y_history[-1] - offset_y_history[-2])
                            low_motion = vel_x <= STABLE_VEL_THRESHOLD and vel_y <= STABLE_VEL_THRESHOLD
                        else:
                            low_motion = False

                        if in_deadband and low_motion:
                            if stable_candidate_since <= 0:
                                stable_candidate_since = current_time
                            elif (
                                (current_time - stable_candidate_since) >= STABLE_HOLD_SEC
                                and (current_time - last_calibration_done_time) >= CALIB_DONE_MIN_INTERVAL_SEC
                            ):
                                broadcast_status("æ ¡å‡†å®Œæˆ", min_interval_sec=0.5)
                                last_calibration_done_time = current_time
                                calibrating_active = False
                                stable_candidate_since = 0.0
                        else:
                            stable_candidate_since = 0.0
                    elif not ptz_control_active:
                        calibrating_active = False
                        stable_candidate_since = 0.0
                        recenter_candidate_count = 0

                    # æ‰‹åŠ¿æ£€æµ‹
                    if gesture_handler and smart_shot_worker:
                        # æ ¡å‡†é˜¶æ®µæš‚ç¼“æ‰‹åŠ¿æ£€æµ‹ï¼Œä¼˜å…ˆä¿è¯ç›®æ ‡è·Ÿéšå®æ—¶æ€§
                        if calibrating_active or recenter_candidate_count > 0:
                            lost_count = 0
                            time.sleep(frame_sleep)
                            continue

                        # ç›®æ ‡åç§»è¾ƒå¤§æ—¶ä¼˜å…ˆäº‘å°è·Ÿéšï¼Œæš‚ç¼“æ‰‹åŠ¿æ¨ç†ï¼Œé¿å…æ‹–æ…¢è·Ÿè¸ª
                        if abs(smoothed_offset_x) > 0.65 or abs(smoothed_offset_y) > 0.75:
                            lost_count = 0
                            time.sleep(frame_sleep)
                            continue

                        x1, y1, x2, y2 = [int(v) for v in main_person.bbox]
                        box_w = max(1, x2 - x1)
                        box_h = max(1, y2 - y1)
                        pad_x = int(box_w * 0.2)
                        pad_y = int(box_h * 0.25)

                        gx1 = max(0, x1 - pad_x)
                        gy1 = max(0, y1 - pad_y)
                        gx2 = min(frame.shape[1], x2 + pad_x)
                        gy2 = min(frame.shape[0], y2 + pad_y)

                        gesture_frame = frame[gy1:gy2, gx1:gx2]
                        if gesture_frame.size == 0:
                            gesture_frame = frame

                        event = gesture_handler.update(gesture_frame, current_time)

                        # å¤„ç†è§¦å‘äº‹ä»¶
                        if event:
                            handle_gesture_event(
                                event,
                                recording_mgr,
                                smart_shot_worker,
                                current_time,
                            )

                    lost_count = 0
                else:
                    # ä¸¢å¤±ç›®æ ‡å¤„ç†
                    recenter_candidate_count = 0
                    offset_x_history.clear()
                    offset_y_history.clear()
                    calibrating_active = False
                    stable_candidate_since = 0.0

                    lost_count += 1

                    if lost_count >= LOST_THRESHOLD:
                        print(f"\n   âš ï¸ ä¸¢å¤±ç›®æ ‡ï¼Œé‡æ–°æ‰«æ...")
                        if enable_miss:
                            broadcast_status("ç›®æ ‡ä¸¢å¤±", min_interval_sec=1.5)
                        if gesture_handler:
                            gesture_handler.reset()
                        if recording_mgr:
                            recording_mgr.on_person_lost()
                        if tracking_mode == "hybrid":
                            software_assist_active = True
                            print("   ğŸ§© hybrid å·²åˆ‡æ¢åˆ°è½¯ä»¶æ‰«æå…œåº•")
                        analyzing = False
                        person_found = False

                time.sleep(frame_sleep)

            else:
                # éåˆ†ææ¨¡å¼ï¼Œä»…æ£€æŸ¥è·Ÿè¸ªçŠ¶æ€
                frame = cam.camera.get_frame()
                if frame is not None:
                    tracks = tracker.update(frame)
                    if tracks:
                        lost_count = 0
                    else:
                        lost_count += 1
                        if lost_count >= LOST_THRESHOLD:
                            if enable_miss:
                                broadcast_status("ç›®æ ‡ä¸¢å¤±", min_interval_sec=1.5)
                            if recording_mgr:
                                recording_mgr.on_person_lost()
                            if tracking_mode == "hybrid":
                                software_assist_active = True
                            person_found = False
                time.sleep(TRACK_CHECK_INTERVAL)

    except KeyboardInterrupt:
        print("\n\nåœæ­¢è¿½è¸ª...")
    finally:
        if voice_queue is not None:
            voice_queue.stop()
        if recording_mgr:
            recording_mgr.cleanup()
        if smart_shot_worker:
            smart_shot_worker.stop()
        cam.camera.stop_stream()
        print("\nè¿½è¸ªå·²åœæ­¢")
        print(f"   å…±æ‰§è¡Œ {cycle_count} è½®")


def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("Mooer Camera NG - æ™ºèƒ½è§†è§’æ§åˆ¶ç³»ç»Ÿ")
    print("\nç”¨æ³•:")
    print("  python3 -m camera_ng --help")
    print("  python3 -m camera_ng <å‘½ä»¤> [é€‰é¡¹] [æ­¥æ•°] [è§’åº¦]")
    print("  python3 -m camera_ng <å‘½ä»¤> --help")
    print("\nå¯ç”¨å‘½ä»¤:")
    print("  human [é€‰é¡¹] [æ­¥æ•°] [è§’åº¦]  - å¤šæ­¥æ‰«ææ‰¾äºº")
    print("  track [é€‰é¡¹] [æ­¥æ•°] [è§’åº¦]  - å®æ—¶è·Ÿè¸ªæ¨¡å¼")
    print("  smart-shot [é€‰é¡¹]           - è·Ÿè¸ª+å³æ‰‹æŠ“æ‹(å¼‚æ­¥ä¿å­˜)+å·¦æ‰‹å½•åƒ")
    print("  shot [æ­¥æ•°] [è§’åº¦]          - æ‹ç…§å¹¶å‘é€")
    print("  prepare-tts [é€‰é¡¹]          - é¢„ç”Ÿæˆå¸¸ç”¨æœ¬æœºæç¤ºéŸ³åˆ° media/tts")
    print("  calibrate                   - æ ¡å‡†äº‘å°è½¬é€Ÿ")
    print("\né€‰é¡¹:")
    print("  -h, --help                  - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
    print("  -g, --gpu                   - ä½¿ç”¨ GPU ç¡¬è§£")
    print("  -quick, --quick             - é«˜æ€§èƒ½æ¨¡å¼ï¼ˆæ›´çµæ•ï¼Œæ›´è€—ç”µï¼‰")
    print("  --tg, --telegram            - å¼€å¯ Telegram å‘é€ï¼ˆé»˜è®¤å…³é—­ï¼‰")
    print("  --enable-miss              - å¼€å¯â€œç›®æ ‡ä¸¢å¤±â€è¯­éŸ³æ’­æŠ¥")
    print("  --voice-drop-oldest         - è¯­éŸ³é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§æ’­æŠ¥")
    print("  --low-latency               - è·Ÿè¸ªæµå¯ç”¨ä½å»¶è¿Ÿæ‹‰æµå‚æ•°")
    print("  --normal-latency            - è·Ÿè¸ªæµä½¿ç”¨å¸¸è§„æ‹‰æµå‚æ•°")
    print("  --ptz-speed <1-7>           - äº‘å°å¸¸è§„é€Ÿåº¦æ¡£ä½")
    print("  --ptz-speed-fast <1-7>      - äº‘å°è¿½èµ¶é€Ÿåº¦æ¡£ä½")
    print("  --tracking-mode <mode>      - è·Ÿè¸ªæ¨¡å¼: native|hybrid|software")
    print("  -m, --mute                  - ä¸å…¥é˜Ÿä»»ä½•æœ¬æœºè¯­éŸ³æ’­æŠ¥")
    print("  --overwrite                 - ä»…ç”¨äº prepare-ttsï¼Œè¦†ç›–å·²æœ‰éŸ³é¢‘")
    print("  --speed <åº¦/ç§’>             - æŒ‡å®šè½¬é€Ÿ")
    print("\nç¤ºä¾‹:")
    print("  python3 -m camera_ng human          # é»˜è®¤æ‰«æ")
    print("  python3 -m camera_ng track -g       # GPU å®æ—¶è·Ÿè¸ª")
    print("  python3 -m camera_ng smart-shot -g -quick  # é«˜çµæ•æ‰‹åŠ¿æŠ“æ‹")
    print("  python3 -m camera_ng smart-shot -g --tg     # å¼€å¯ Telegram å‘é€")
    print("  python3 -m camera_ng track -g --low-latency # ä½å»¶è¿Ÿè·Ÿè¸ª")
    print("  python3 -m camera_ng track --tracking-mode hybrid")
    print("  python3 -m camera_ng smart-shot -g --tracking-mode native")
    print("  python3 -m camera_ng track --ptz-speed 2 --ptz-speed-fast 5")
    print("  python3 -m camera_ng shot 8 180 --tg         # æ‹ç…§å¹¶å‘é€")
    print("  python3 -m camera_ng prepare-tts             # é¢„ç”Ÿæˆæœ¬åœ°æç¤ºéŸ³")
    print("\nSmart-Shot è¡Œä¸º:")
    print("  - å³æ‰‹æŠ¬èµ·ï¼šå¼‚æ­¥ä¿å­˜é«˜è´¨é‡ç…§ç‰‡åˆ° ~/Desktop/capture/pictures/<timestamp>.jpg")
    print("  - å·¦æ‰‹æŠ¬èµ·ï¼šå¼€å§‹/åœæ­¢å½•åƒ")
    print("  - ç›®æ ‡æ•è·/æ ¡å‡†ä¸­/æ ¡å‡†å®Œæˆï¼šé»˜è®¤æ’­æŠ¥")
    print("  - ç›®æ ‡ä¸¢å¤±ï¼šä»… --enable-miss æ—¶æ’­æŠ¥")
    print("  - è¯­éŸ³æ’­æŠ¥ç»Ÿä¸€å¼‚æ­¥é˜Ÿåˆ—ï¼›å¯ç”¨ --voice-drop-oldest é¿å…ç§¯å‹")
    print("  - ä½¿ç”¨ -m/--mute å¯ç¦ç”¨å…¨éƒ¨æœ¬æœºè¯­éŸ³å…¥é˜Ÿ")
    print("  - Telegram é»˜è®¤å…³é—­ï¼›åŠ  --tg æ‰å‘é€")


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    if len(sys.argv) < 2 or sys.argv[1] in ('-h', '--help'):
        show_help()
        sys.exit(0 if sys.argv[1] in ('-h', '--help') else 1)

    cmd = sys.argv[1]
    args = sys.argv[2:]

    if "-h" in args or "--help" in args:
        show_help()
        sys.exit(0)

    # è§£æ GPU é€‰é¡¹
    use_gpu = False
    if "-g" in args or "--gpu" in args:
        use_gpu = True
        args = [a for a in args if a not in ["-g", "--gpu"]]

    # è§£æé«˜æ€§èƒ½é€‰é¡¹
    quick_mode = False
    if "-quick" in args or "--quick" in args:
        quick_mode = True
        args = [a for a in args if a not in ["-quick", "--quick"]]

    # è§£æ Telegram å‘é€é€‰é¡¹
    send_to_tg = False
    if "--tg" in args or "--telegram" in args:
        send_to_tg = True
        args = [a for a in args if a not in ["--tg", "--telegram"]]

    overwrite_tts = False
    if "--overwrite" in args:
        overwrite_tts = True
        args = [a for a in args if a != "--overwrite"]

    enable_miss = False
    if "--enable-miss" in args:
        enable_miss = True
        args = [a for a in args if a != "--enable-miss"]

    voice_drop_oldest = False
    if "--voice-drop-oldest" in args:
        voice_drop_oldest = True
        args = [a for a in args if a != "--voice-drop-oldest"]

    mute_voice = False
    if "-m" in args or "--mute" in args:
        mute_voice = True
        args = [a for a in args if a not in ["-m", "--mute"]]

    low_latency_stream = STREAM_LOW_LATENCY
    if "--low-latency" in args:
        low_latency_stream = True
        args = [a for a in args if a != "--low-latency"]
    if "--normal-latency" in args:
        low_latency_stream = False
        args = [a for a in args if a != "--normal-latency"]

    ptz_speed = PTZ_SPEED
    if "--ptz-speed" in args:
        idx = args.index("--ptz-speed")
        if idx + 1 < len(args):
            ptz_speed = max(1, min(7, int(args[idx + 1])))
            args = args[:idx] + args[idx + 2:]

    ptz_speed_fast = PTZ_FAST_SPEED
    if "--ptz-speed-fast" in args:
        idx = args.index("--ptz-speed-fast")
        if idx + 1 < len(args):
            ptz_speed_fast = max(1, min(7, int(args[idx + 1])))
            args = args[:idx] + args[idx + 2:]

    tracking_mode = "hybrid"
    if "--tracking-mode" in args:
        idx = args.index("--tracking-mode")
        if idx + 1 < len(args):
            tracking_mode = args[idx + 1].strip().lower()
            args = args[:idx] + args[idx + 2:]
        else:
            print("âŒ --tracking-mode éœ€è¦å‚æ•°: native|hybrid|software")
            sys.exit(1)

    if tracking_mode not in {"native", "hybrid", "software"}:
        print(f"âŒ æ— æ•ˆ --tracking-mode: {tracking_mode}")
        print("   å¯é€‰å€¼: native, hybrid, software")
        sys.exit(1)

    # è§£æè½¬é€Ÿé€‰é¡¹
    global ROTATION_SPEED
    if "--speed" in args:
        speed_idx = args.index("--speed")
        if speed_idx + 1 < len(args):
            ROTATION_SPEED = float(args[speed_idx + 1])
            print(f"âš™ï¸  ä½¿ç”¨æŒ‡å®šè½¬é€Ÿ: {ROTATION_SPEED}Â°/s")
            args = args[:speed_idx] + args[speed_idx + 2:]

    num_steps = int(args[0]) if len(args) > 0 and args[0].lstrip("-").isdigit() else DEFAULT_NUM_STEPS
    total_angle = float(args[1]) if len(args) > 1 and args[1].replace(".", "", 1).lstrip("-").isdigit() else DEFAULT_TOTAL_ANGLE

    config_required_cmds = {"human", "shot", "track", "smart-shot"}
    if cmd in config_required_cmds:
        validate_config()

    lock = check_single_instance() if cmd in config_required_cmds else None

    cam = None
    try:
        if cmd == "human":
            cam = SmartCamera()
            result = cam.human(num_steps=num_steps, total_angle=total_angle, use_gpu=use_gpu)
            print(f"\n{'='*60}")
            print(f"æ‰«æç»“æœ: {'æ‰¾åˆ°äºº' if result else 'æœªæ‰¾åˆ°äºº'}")
            print(f"{'='*60}")
            cam.stop()
            
        elif cmd == "shot":
            cam = SmartCamera()
            result = cam.human(num_steps=num_steps, total_angle=total_angle, 
                             use_gpu=use_gpu, center_and_wait=True)
            if result:
                capture_and_send_current_view(
                    cam.camera,
                    "Albertï¼Œæˆ‘æŠ“æ‹åˆ°ä½ å•¦ï¼ğŸ“¸ğŸ’•",
                    send_to_tg=send_to_tg,
                )
                    
            print(f"\n{'='*60}")
            print(f"æ‹ç…§ç»“æœ: {'æˆåŠŸ' if result else 'æœªæ‰¾åˆ°äºº'}")
            print(f"{'='*60}")
            cam.stop()
            
        elif cmd == "track":
            track_human_realtime(
                num_steps=num_steps,
                total_angle=total_angle,
                use_gpu=use_gpu,
                tracking_mode=tracking_mode,
                quick_mode=quick_mode,
                send_to_tg=send_to_tg,
                enable_miss=enable_miss,
                voice_drop_oldest=voice_drop_oldest,
                mute_voice=mute_voice,
                low_latency_stream=low_latency_stream,
                ptz_speed=ptz_speed,
                ptz_speed_fast=ptz_speed_fast,
            )

        elif cmd == "smart-shot":
            track_human_realtime(
                num_steps=num_steps,
                total_angle=total_angle,
                use_gpu=use_gpu,
                tracking_mode=tracking_mode,
                smart_shot=True,
                quick_mode=quick_mode,
                send_to_tg=send_to_tg,
                enable_miss=enable_miss,
                voice_drop_oldest=voice_drop_oldest,
                mute_voice=mute_voice,
                low_latency_stream=low_latency_stream,
                ptz_speed=ptz_speed,
                ptz_speed_fast=ptz_speed_fast,
            )

        elif cmd == "prepare-tts":
            tts = XiaoxiaoTTS()
            created, skipped = tts.pregenerate_common_prompts(overwrite=overwrite_tts)
            print("\n" + "=" * 60)
            print("ğŸ”Š é¢„ç”Ÿæˆæç¤ºéŸ³å®Œæˆ")
            print(f"   ç›®å½•: {tts.media_dir}")
            print(f"   æ–°ç”Ÿæˆ: {created}")
            print(f"   è·³è¿‡: {skipped}")
            print("=" * 60)
            
        elif cmd == "calibrate":
            subprocess.run(["python3", "/home/albert/clawd/scripts/calibrate_speed.py"])
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {cmd}")
            print("æ”¯æŒå‘½ä»¤: human, shot, track, smart-shot, prepare-tts, calibrate")
            sys.exit(1)
    finally:
        if lock is not None:
            lock.release()


if __name__ == "__main__":
    main()
