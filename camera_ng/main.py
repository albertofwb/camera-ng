#!/usr/bin/env python3
"""
Mooer Camera NG - ä¸»å…¥å£
é‡æ„åçš„æ¨¡å—åŒ–æ™ºèƒ½è§†è§’æ§åˆ¶ç³»ç»Ÿ
"""

import faulthandler
import fcntl
import os
import queue
import subprocess
import sys
import threading
import time
from collections import deque

# å¯ç”¨ faulthandlerï¼Œåœ¨å´©æºƒæ—¶æ‰“å° Python å †æ ˆ
faulthandler.enable()

from camera_ng import (
    DEFAULT_NUM_STEPS, DEFAULT_TOTAL_ANGLE,
    ROTATION_SPEED, TRACK_CHECK_INTERVAL, DETECTION_INTERVAL,
    TRACKER_MAX_AGE, TRACKER_MIN_HITS,
    CAPTURE_WIDTH, CAPTURE_HEIGHT, LOCK_FILE,
    CAMERA_RTSP, DEVICE_SERIAL, ACCESS_TOKEN,
    CameraController, VisionAnalyzer, HandRaiseDetector, XiaoxiaoTTS,
    PersonTracker, TrackingMemory
)

TELEGRAM_TARGET = "1115213761"


def run_openclaw_send(cmd: list[str], retries: int = 2, timeout_sec: int = 20) -> bool:
    """å‘é€æ¶ˆæ¯ï¼ˆå«è¶…æ—¶ä¸é‡è¯•ï¼‰ï¼Œé¿å…åå°ä»»åŠ¡é•¿æœŸå ç”¨"""
    for attempt in range(1, retries + 1):
        try:
            subprocess.run(cmd, check=True, timeout=timeout_sec)
            return True
        except Exception as e:
            if attempt == retries:
                print(f"âŒ OpenClaw å‘é€å¤±è´¥: {e}")
                return False
            print(f"âš ï¸ OpenClaw å‘é€å¤±è´¥ï¼Œé‡è¯• {attempt}/{retries - 1}: {e}")
            time.sleep(0.6)
    return False


def validate_config():
    """éªŒè¯å¿…è¦é…ç½®æ˜¯å¦å­˜åœ¨"""
    errors = []
    
    if CAMERA_RTSP is None:
        errors.append("  - camera.rtsp_url: RTSP æµåœ°å€æœªé…ç½®")
    if DEVICE_SERIAL is None:
        errors.append("  - camera.device_serial: è®¾å¤‡åºåˆ—å·æœªé…ç½®")
    if ACCESS_TOKEN is None:
        errors.append("  - camera.access_token: è¤çŸ³äº‘ AccessToken æœªé…ç½®")
    
    if errors:
        print("\n" + "=" * 60)
        print("âŒ é…ç½®é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„æ•æ„Ÿä¿¡æ¯")
        print("=" * 60)
        print("\nè¯·åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€åˆ›å»º camera-config.yaml æ–‡ä»¶ï¼š")
        print("  1. ./memory/camera-config.yaml")
        print("  2. ~/.openclaw/workspace/memory/camera-config.yaml")
        print("  3. ~/clawd/memory/camera-config.yaml")
        print("  4. ~/.config/mooer-camera/config.yaml")
        print("\nç¼ºå¤±çš„é…ç½®é¡¹ï¼š")
        for err in errors:
            print(err)
        print("\ncamera-config.yaml ç¤ºä¾‹æ ¼å¼ï¼š")
        print("""
camera:
  rtsp_url: "rtsp://admin:PASSWORD@IP:554/h264/ch1/main/av_stream"
  device_serial: "YOUR_DEVICE_SERIAL"
  access_token: "YOUR_ACCESS_TOKEN"
  capture:
    seek_time: "00:00:00.5"
    resolution:
      width: 640
      height: 360
    quality: 2
  ptz:
    rotation_speed: 28
""")
        print("=" * 60)
        sys.exit(1)


# å¯åŠ¨æ—¶éªŒè¯é…ç½®
validate_config()


class SmartCamera:
    """æ™ºèƒ½æ‘„åƒå¤´ç³»ç»Ÿï¼ˆhuman ä¸“ç”¨ï¼‰"""

    def __init__(self):
        self.camera = CameraController()
        self.vision = VisionAnalyzer()

    def human(self, num_steps: int = DEFAULT_NUM_STEPS,
              total_angle: float = DEFAULT_TOTAL_ANGLE,
              use_gpu: bool = False, smart: bool = True,
              fast: bool = False, keep_stream: bool = True,
              center_and_wait: bool = False) -> bool:
        """æ‰§è¡Œ human æ‰«æï¼Œæ‰¾åˆ°äººè¿”å›True"""
        if not self.camera.stream_active:
            print("ğŸ¥ å¯åŠ¨è§†é¢‘æµ...")
            if not self.camera.start_stream(use_gpu=use_gpu):
                print("âŒ æ— æ³•å¯åŠ¨è§†é¢‘æµ")
                return False
            time.sleep(0.5)
        else:
            print("ğŸ”„ å¤ç”¨å·²æœ‰è§†é¢‘æµ...")

        try:
            self.camera.center_and_wait_mode = center_and_wait
            
            if smart:
                result = self.camera.human_steps_smart(self.vision)
            elif fast:
                result = self.camera.human_steps_fast(self.vision)
            else:
                result = self.camera.human_steps(
                    self.vision, num_steps=num_steps, total_angle=total_angle
                )
        finally:
            if not keep_stream:
                self.camera.stop_stream()

        return result
    
    def stop(self):
        """å®Œå…¨åœæ­¢å¹¶æ¸…ç†èµ„æº"""
        self.camera.stop_stream()
        print("âœ… æ™ºèƒ½æ‘„åƒå¤´ç³»ç»Ÿå·²åœæ­¢")
    
    def human_smart_only(self) -> bool:
        """æ™ºèƒ½æ‰«æï¼ˆå¤ç”¨å·²æœ‰æµï¼‰"""
        return self.human(smart=True)


class SingleInstanceLock:
    """å•å®ä¾‹æ–‡ä»¶é”"""

    def __init__(self, lock_file: str = LOCK_FILE):
        self.lock_file = lock_file
        self.fd = None

    def acquire(self) -> bool:
        """è·å–æ–‡ä»¶é”"""
        try:
            self.fd = open(self.lock_file, "w")
            fcntl.flock(self.fd.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            self.fd.write(str(os.getpid()))
            self.fd.flush()
            return True
        except (IOError, OSError):
            if self.fd:
                self.fd.close()
                self.fd = None
            return False

    def release(self):
        """é‡Šæ”¾æ–‡ä»¶é”"""
        if self.fd:
            try:
                fcntl.flock(self.fd.fileno(), fcntl.LOCK_UN)
                self.fd.close()
                if os.path.exists(self.lock_file):
                    os.remove(self.lock_file)
            except (IOError, OSError):
                pass
            finally:
                self.fd = None

    def __enter__(self):
        if not self.acquire():
            raise RuntimeError("å¦ä¸€ä¸ªå®ä¾‹æ­£åœ¨è¿è¡Œ")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()


def capture_and_send_current_view(camera: CameraController, message: str) -> bool:
    """åŸºäºå½“å‰ç”»é¢ç›´æ¥æŠ“æ‹å¹¶å‘é€ï¼Œä¸æ‰§è¡Œæ‰¾äººæµç¨‹"""
    img_path = camera.capture(full_quality=True)
    print(f"ğŸ“¸ å·²æŠ“æ‹å½“å‰ç”»é¢: {img_path}")

    try:
        send_cmd = [
            "openclaw", "message", "send",
            "--channel", "telegram",
            "--target", TELEGRAM_TARGET,
            "--media", img_path,
            "--message", message
        ]
        print("ğŸ“¤ æ­£åœ¨é€šè¿‡ OpenClaw å‘é€ç…§ç‰‡...")
        ok = run_openclaw_send(send_cmd)
        if ok:
            print("âœ… ç…§ç‰‡å‘é€æˆåŠŸï¼")
        return ok
    except Exception as e:
        print(f"âŒ ç…§ç‰‡å‘é€å¤±è´¥: {e}")
        return False


def send_greeting_voice(tts: XiaoxiaoTTS, message: str) -> bool:
    """å‘é€ä¸­æ–‡é—®å€™è¯­éŸ³åˆ°ä¸å›¾ç‰‡ç›¸åŒçš„ Telegram ç›®æ ‡"""
    try:
        if tts.playback(message):
            print("ğŸ”ˆ å·²åœ¨æœ¬æœºæ’­æ”¾é—®å€™è¯­éŸ³")
        else:
            print("âš ï¸ æœ¬æœºè¯­éŸ³æ’­æ”¾å¤±è´¥ï¼ˆå·²ç»§ç»­å‘é€ Telegram è¯­éŸ³ï¼‰")

        voice_path = tts.synthesize(message)
        send_cmd = [
            "openclaw", "message", "send",
            "--channel", "telegram",
            "--target", TELEGRAM_TARGET,
            "--media", voice_path,
            "--message", "å³æ‰‹æ‰‹åŠ¿è¯­éŸ³é—®å€™",
        ]
        print("ğŸ”Š æ­£åœ¨å‘é€é—®å€™è¯­éŸ³...")
        ok = run_openclaw_send(send_cmd)
        if ok:
            print("âœ… è¯­éŸ³å‘é€æˆåŠŸï¼")
        return ok
    except Exception as e:
        print(f"âŒ è¯­éŸ³å‘é€å¤±è´¥: {e}")
        return False


def start_high_quality_recording() -> tuple[subprocess.Popen[str] | None, str | None]:
    """å¯åŠ¨é«˜è´¨é‡å½•åƒå¹¶è¿”å› ffmpeg è¿›ç¨‹ä¸è¾“å‡ºè·¯å¾„"""
    try:
        output_dir = os.path.expanduser("~/Desktop/capture")
        os.makedirs(output_dir, exist_ok=True)
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(output_dir, f"{timestamp}.mp4")

        cmd = [
            "ffmpeg",
            "-rtsp_transport",
            "tcp",
            "-i",
            CAMERA_RTSP,
            "-c",
            "copy",
            "-movflags",
            "+faststart",
            "-y",
            output_path,
        ]
        proc = subprocess.Popen(
            cmd,
            stdin=subprocess.PIPE,
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            text=True,
        )
        print(f"ğŸ¬ å·¦æ‰‹è§¦å‘ï¼šå¼€å§‹å½•åƒ {output_path}")
        return proc, output_path
    except Exception as e:
        print(f"âŒ å¯åŠ¨å½•åƒå¤±è´¥: {e}")
        return None, None


def stop_high_quality_recording(record_proc: subprocess.Popen[str] | None, output_path: str | None) -> None:
    """åœæ­¢å½•åƒå¹¶è½ç›˜"""
    if record_proc is None:
        return

    proc = record_proc
    try:
        if proc.poll() is None:
            if proc.stdin is not None:
                proc.stdin.write("q\n")
                proc.stdin.flush()
            proc.wait(timeout=2.0)
    except Exception:
        try:
            proc.terminate()
            proc.wait(timeout=1.5)
        except Exception:
            try:
                proc.kill()
            except Exception:
                pass
    finally:
        if output_path:
            print(f"âœ… å·¦æ‰‹è§¦å‘ï¼šåœæ­¢å½•åƒï¼Œå·²ä¿å­˜ {output_path}")


def local_voice_broadcast(tts: XiaoxiaoTTS | None, text: str) -> None:
    """æœ¬æœºè¯­éŸ³æ’­æŠ¥ï¼ˆåå°æ‰§è¡Œï¼Œä¸é˜»å¡å½•åƒ/è¿½è¸ªï¼‰"""
    if tts is None or not tts.is_available():
        return

    def _worker() -> None:
        try:
            ok = tts.playback(text)
            if ok:
                print(f"ğŸ”ˆ è¯­éŸ³æ’­æŠ¥: {text}")
        except Exception:
            pass

    threading.Thread(target=_worker, daemon=True).start()


def start_smart_shot_worker(
    camera: CameraController,
    tts: XiaoxiaoTTS | None,
    task_queue: queue.Queue,
    stop_event: threading.Event,
) -> threading.Thread:
    """å¯åŠ¨ Smart-Shot åå° workerï¼ˆä¸²è¡Œæ¶ˆè´¹é˜Ÿåˆ—ä»»åŠ¡ï¼‰"""

    def _worker():
        while not stop_event.is_set():
            try:
                hand_text, hand_reason = task_queue.get(timeout=0.2)
            except queue.Empty:
                continue

            try:
                capture_and_send_current_view(
                    camera,
                    f"Albertï¼Œæˆ‘æ£€æµ‹åˆ°ä½ æŠ¬{hand_text}ï¼Œå·²ä¸ºä½ æŠ“æ‹ï¼ğŸ“¸",
                )
                if "right" in hand_reason and tts is not None and tts.is_available():
                    send_greeting_voice(tts, "å—¨ Albertï¼Œä½ å¥½å‘€ï¼Œæˆ‘çœ‹åˆ°ä½ ä¸¾èµ·å³æ‰‹å•¦ã€‚")
            finally:
                task_queue.task_done()

    worker = threading.Thread(target=_worker, daemon=True)
    worker.start()
    return worker


def trigger_smart_shot_async(
    hand_text: str,
    hand_reason: str,
    tts: XiaoxiaoTTS | None,
    task_queue: queue.Queue,
) -> bool:
    """å…¥é˜Ÿ Smart-Shot ä»»åŠ¡ï¼Œä¸»å¾ªç¯ä¸é˜»å¡ï¼ˆæ»¡é˜Ÿåˆ—æ—¶ä¸¢å¼ƒæœ€æ—§ä»»åŠ¡ï¼‰"""

    if task_queue.full():
        try:
            _ = task_queue.get_nowait()
            task_queue.task_done()
            print("ğŸ—‘ï¸ Smart-Shot é˜Ÿåˆ—å·²æ»¡ï¼Œä¸¢å¼ƒæœ€æ—§ä»»åŠ¡ï¼ˆdrop_oldestï¼‰")
        except queue.Empty:
            pass

    try:
        task_queue.put_nowait((hand_text, hand_reason))
        if tts is not None and tts.is_available():
            if tts.playback("æ”¶åˆ°"):
                print("ğŸ”ˆ å·²æœ¬æœºæ’­æŠ¥: æ”¶åˆ°")
            else:
                print("âš ï¸ æœ¬æœºæ’­æŠ¥â€œæ”¶åˆ°â€å¤±è´¥")
        return True
    except queue.Full:
        print("â³ Smart-Shot é˜Ÿåˆ—æ‹¥å¡ï¼Œè·³è¿‡æœ¬æ¬¡è§¦å‘")
        return False


def check_single_instance():
    """æ£€æŸ¥æ˜¯å¦å•å®ä¾‹è¿è¡Œ"""
    lock = SingleInstanceLock()
    if not lock.acquire():
        print("âŒ é”™è¯¯ï¼šå¦ä¸€ä¸ª mooer_camera å®ä¾‹æ­£åœ¨è¿è¡Œ")
        print(f"   é”æ–‡ä»¶: {LOCK_FILE}")
        print("   è¯·å…ˆåœæ­¢å…¶ä»–å®ä¾‹å†è¿è¡Œ")
        sys.exit(1)
    return lock


def track_human_realtime(num_steps: int = DEFAULT_NUM_STEPS,
                         total_angle: float = 360,
                         detection_interval: int = DETECTION_INTERVAL,
                         use_gpu: bool = False,
                         smart_shot: bool = False,
                         quick_mode: bool = False) -> None:
    """å®æ—¶ç›®æ ‡è·Ÿè¸ªæ¨¡å¼"""
    cam = SmartCamera()
    effective_detection_interval = 1 if quick_mode else detection_interval
    tracker = PersonTracker(
        yolo_model="yolov8n",
        confidence=0.5,
        detection_interval=effective_detection_interval
    )
    
    cycle_count = 0
    person_found = False
    analyzing = False
    lost_count = 0
    LOST_THRESHOLD = 5
    
    fps_history = deque(maxlen=30)
    last_time = time.time()
    offset_x_history = deque(maxlen=5)
    offset_y_history = deque(maxlen=5)
    recenter_candidate_count = 0
    last_recenter_time = 0.0

    # æŠ—æŠ–å‚æ•°ï¼šé¿å…â€œè½¬å¤´/å–æ°´â€è¿™ç±»çŸ­æ—¶å§¿æ€å˜åŒ–è§¦å‘äº‘å°
    RECENTER_CONFIRM_FRAMES = 3
    RECENTER_COOLDOWN = 1.2
    BASE_RECENTER_X_THRESHOLD = 0.5
    BASE_RECENTER_Y_THRESHOLD = 0.6

    hand_raise_detector = HandRaiseDetector() if smart_shot else None
    tts = XiaoxiaoTTS() if smart_shot else None
    smart_shot_queue = queue.Queue(maxsize=3) if smart_shot else None
    smart_shot_stop_event = threading.Event() if smart_shot else None
    smart_shot_worker = (
        start_smart_shot_worker(cam.camera, tts, smart_shot_queue, smart_shot_stop_event)
        if smart_shot and smart_shot_queue is not None and smart_shot_stop_event is not None
        else None
    )
    hand_raise_confirm_frames = 1 if quick_mode else 2
    hand_raise_count = 0
    shot_cooldown = 0.6 if quick_mode else 1.0
    last_shot_time = 0.0
    last_hand_log_time = 0.0
    hand_trigger_armed = True
    record_proc: subprocess.Popen[str] | None = None
    record_output_path: str | None = None
    record_toggle_cooldown = 1.5
    last_record_toggle_time = 0.0
    frame_sleep = 0.005 if quick_mode else 0.03
    recenter_pause = 0.2 if quick_mode else 0.5

    print("\n" + "=" * 60)
    print("ğŸ” å¯åŠ¨å®æ—¶ç›®æ ‡è·Ÿè¸ªæ¨¡å¼ (Real-time + SORT)")
    print("=" * 60)
    print(f"é…ç½®: {num_steps}æ­¥/{total_angle}Â°")
    print(f"YOLOæ£€æµ‹é—´éš”: æ¯{effective_detection_interval}å¸§")
    print(f"è·Ÿè¸ªå™¨: SORT (max_age={TRACKER_MAX_AGE}, min_hits={TRACKER_MIN_HITS})")
    print(f"è§†é¢‘è§£ç : {'GPU (CUDA)' if use_gpu else 'CPU'}")
    if smart_shot:
        print("ğŸ“¸ Smart-Shot: å³æ‰‹æŠ¬èµ·æŠ“æ‹å‘é€ï¼Œå·¦æ‰‹æŠ¬èµ·å¼€å§‹/åœæ­¢å½•åƒ")
        if hand_raise_detector is None or hand_raise_detector.model is None:
            print("âš ï¸ Smart-Shot pose æ¨¡å‹ä¸å¯ç”¨ï¼ŒæŠ¬æ‰‹æ£€æµ‹ä¸ä¼šè§¦å‘")
        if tts is None or not tts.is_available():
            print("âš ï¸ æ™“æ™“ TTS ä¸å¯ç”¨ï¼Œå³æ‰‹æŠ¬èµ·åä¸ä¼šå‘é€è¯­éŸ³")
        print("ğŸ“¬ Smart-Shot é˜Ÿåˆ—ç­–ç•¥: drop_oldestï¼ˆé˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§ä»»åŠ¡ï¼‰")
    if quick_mode:
        print("âš¡ Quick æ¨¡å¼: é«˜é¢‘æ£€æµ‹ + æ›´ä½å†·é™æ—¶é—´")
    print("æŒ‰ Ctrl+C åœæ­¢è¿½è¸ª")
    print("=" * 60 + "\n")

    if not cam.camera.start_stream(use_gpu=use_gpu):
        print("âŒ æ— æ³•å¯åŠ¨è§†é¢‘æµ")
        return

    try:
        while True:
            cycle_count += 1
            current_time = time.time()
            
            fps_history.append(1.0 / (current_time - last_time + 0.001))
            avg_fps = sum(fps_history) / len(fps_history)
            last_time = current_time

            if not person_found:
                print(f"\n{'=' * 60}")
                print(f"ğŸ”„ ç¬¬ {cycle_count} è½® | æ‰§è¡Œæ™ºèƒ½æ‰«æ...")
                print(f"{'=' * 60}")

                person_found = cam.human_smart_only()

                if person_found:
                    print("âœ… æ‰¾åˆ°ç›®æ ‡ï¼")
                    cam.camera.tracking_memory.reset()
                    
                    if not cam.camera.stream_active:
                        if not cam.camera.start_stream():
                            return
                        time.sleep(0.5)

                    init_attempts = 0
                    max_init_attempts = 10
                    
                    while init_attempts < max_init_attempts:
                        frame = cam.camera.get_frame()
                        if frame is not None:
                            tracks = tracker.update(frame, force_detect=True)
                            if tracks:
                                analyzing = True
                                lost_count = 0
                                break
                        init_attempts += 1
                        time.sleep(0.1)
                    else:
                        person_found = False
                        continue

                    analyzing = True
                    lost_count = 0
                else:
                    print("æœªæ‰¾åˆ°ï¼Œç»§ç»­æ‰«æ...")
                    if not cam.camera.start_stream():
                        cam.camera.start_stream()
                    time.sleep(0.5)
                    
            elif analyzing:
                # å®æ—¶è·Ÿè¸ªæ¨¡å¼ - å¼ºåˆ¶å¼‚æ­¥è¯»å–æœ€æ–°å¸§
                frame = cam.camera.get_frame()
                if frame is None:
                    time.sleep(0.001) # æçŸ­ç­‰å¾…ï¼Œé˜²æ­¢ç©ºå¾ªç¯
                    continue
                
                # æ›´æ–°è·Ÿè¸ªå™¨ (YOLOæ¨ç†å®Œå…¨åœ¨GPUä¸Šï¼Œä¸é˜»å¡æ‹‰æµçº¿ç¨‹)
                tracks = tracker.update(frame)
                main_person = tracker.get_main_person()
                
                # è®¡ç®—å¹¶æ˜¾ç¤º FPS (è¯†åˆ«å¸§ç‡)
                detect_mode = "DETECT" if cycle_count % effective_detection_interval == 0 else "TRACK "
                status = f"\rğŸ“Š [{detect_mode}] FPS:{avg_fps:.1f} | Tracks:{len(tracks)}"
                if main_person:
                    cx, cy = main_person.bbox[[0, 2]].mean(), main_person.bbox[[1, 3]].mean()
                    offset_x = (cx - CAPTURE_WIDTH/2) / (CAPTURE_WIDTH/2)
                    status += f" | MainID:{main_person.id} offset:{offset_x:+.2f}"
                print(status, end="", flush=True)
                
                if main_person:
                    cx = (main_person.bbox[0] + main_person.bbox[2]) / 2
                    cy = (main_person.bbox[1] + main_person.bbox[3]) / 2
                    offset_x = (cx - CAPTURE_WIDTH/2) / (CAPTURE_WIDTH/2)
                    offset_y = (cy - CAPTURE_HEIGHT/2) / (CAPTURE_HEIGHT/2)

                    offset_x_history.append(offset_x)
                    offset_y_history.append(offset_y)
                    smoothed_offset_x = sum(offset_x_history) / len(offset_x_history)
                    smoothed_offset_y = sum(offset_y_history) / len(offset_y_history)
                    
                    # æ›´æ–°è¿åŠ¨è®°å¿†
                    current_angle = cam.camera.tracking_memory.last_angle
                    if smoothed_offset_x < -0.3:
                        current_angle = (current_angle - 20) % 360
                    elif smoothed_offset_x > 0.3:
                        current_angle = (current_angle + 20) % 360
                    cam.camera.tracking_memory.update(current_angle)
                    
                    # è§¦å‘å±…ä¸­é€»è¾‘ï¼ˆè¿ç»­å¤šå¸§ + å¹³æ»‘ + å†·å´ï¼‰
                    person_width_ratio = (main_person.bbox[2] - main_person.bbox[0]) / CAPTURE_WIDTH
                    dynamic_x_threshold = BASE_RECENTER_X_THRESHOLD + min(0.25, person_width_ratio * 0.35)
                    need_recenter = (
                        abs(smoothed_offset_x) > dynamic_x_threshold
                        or abs(smoothed_offset_y) > BASE_RECENTER_Y_THRESHOLD
                    )

                    if need_recenter:
                        recenter_candidate_count += 1
                    else:
                        recenter_candidate_count = 0

                    if (
                        recenter_candidate_count >= RECENTER_CONFIRM_FRAMES
                        and (current_time - last_recenter_time) >= RECENTER_COOLDOWN
                    ):
                        print(
                            f"\n   ğŸ¯ æŒç»­åç§»è§¦å‘å±…ä¸­: æ°´å¹³{smoothed_offset_x:+.2f}, å‚ç›´{smoothed_offset_y:+.2f}"
                        )
                        cam.camera.center_person(smoothed_offset_x, smoothed_offset_y)
                        recenter_candidate_count = 0
                        last_recenter_time = current_time
                        offset_x_history.clear()
                        offset_y_history.clear()
                        # ä»…åœ¨è°ƒæ•´äº‘å°åçŸ­æš‚åœé¡¿ï¼Œå…¶ä»–æ—¶é—´å…¨åŠ›è·‘
                        time.sleep(recenter_pause)

                    if smart_shot and hand_raise_detector is not None and smart_shot_queue is not None:
                        hand_raised, hand_reason = hand_raise_detector.get_hand_raise_state(frame)
                        if hand_raised:
                            hand_raise_count = min(hand_raise_count + 1, hand_raise_confirm_frames)
                        else:
                            hand_raise_count = max(hand_raise_count - 1, 0)
                            if hand_raise_count == 0:
                                hand_trigger_armed = True

                        if (current_time - last_hand_log_time) >= 2.0:
                            print(f"\n   ğŸ™‹ æ‰‹åŠ¿æ£€æµ‹: {hand_reason} | è¿ç»­å¸§: {hand_raise_count}/{hand_raise_confirm_frames}")
                            last_hand_log_time = current_time

                        if (
                            hand_trigger_armed
                            and hand_raised
                            and hand_raise_count >= hand_raise_confirm_frames
                            and (current_time - last_shot_time) >= shot_cooldown
                        ):
                            if "left" in hand_reason:
                                if (current_time - last_record_toggle_time) >= record_toggle_cooldown:
                                    if record_proc is None or record_proc.poll() is not None:
                                        record_proc, record_output_path = start_high_quality_recording()
                                        if record_proc is not None:
                                            local_voice_broadcast(tts, "å¼€å§‹å½•åƒ")
                                    else:
                                        stop_high_quality_recording(record_proc, record_output_path)
                                        local_voice_broadcast(tts, "åœæ­¢å½•åƒ")
                                        record_proc = None
                                        record_output_path = None
                                    last_record_toggle_time = current_time
                                hand_raise_count = 0
                                hand_trigger_armed = False
                                continue

                            if "right" not in hand_reason:
                                hand_raise_count = 0
                                hand_trigger_armed = False
                                continue

                            print("\n   ğŸ™‹ æ£€æµ‹åˆ°å³æ‰‹æŠ¬èµ·ï¼Œè¿›å…¥ Smart-Shotï¼ˆä¸é‡æ–°æ‰¾äººï¼‰...")
                            started = trigger_smart_shot_async(
                                "å³æ‰‹",
                                hand_reason,
                                tts,
                                smart_shot_queue,
                            )
                            if started:
                                last_shot_time = current_time
                                hand_raise_count = 0
                                hand_trigger_armed = False
                    
                    lost_count = 0
                else:
                    recenter_candidate_count = 0
                    offset_x_history.clear()
                    offset_y_history.clear()
                    lost_count += 1
                    if lost_count >= LOST_THRESHOLD:
                        hand_raise_count = 0
                        print(f"\n   âš ï¸ ä¸¢å¤±ç›®æ ‡ï¼Œé‡æ–°æ‰«æ...")
                        analyzing = False
                        person_found = False
                
                # é™åˆ¶ FPS é¿å… CPU å¿™ç­‰
                time.sleep(frame_sleep)
            else:
                frame = cam.camera.get_frame()
                if frame is not None:
                    tracks = tracker.update(frame)
                    if tracks:
                        lost_count = 0
                    else:
                        lost_count += 1
                        if lost_count >= LOST_THRESHOLD:
                            person_found = False
                time.sleep(TRACK_CHECK_INTERVAL)

    except KeyboardInterrupt:
        print("\n\nåœæ­¢è¿½è¸ª...")
    finally:
        if record_proc is not None:
            stop_high_quality_recording(record_proc, record_output_path)
        if smart_shot_stop_event is not None:
            smart_shot_stop_event.set()
        if smart_shot_worker is not None:
            smart_shot_worker.join(timeout=0.5)
        cam.camera.stop_stream()
        print("\nè¿½è¸ªå·²åœæ­¢")
        print(f"   å…±æ‰§è¡Œ {cycle_count} è½®")


def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("Mooer Camera NG - æ™ºèƒ½è§†è§’æ§åˆ¶ç³»ç»Ÿ")
    print("\nç”¨æ³•: python3 -m camera_ng <å‘½ä»¤> [é€‰é¡¹] [æ­¥æ•°] [è§’åº¦]")
    print("\nå¯ç”¨å‘½ä»¤:")
    print("  human [é€‰é¡¹] [æ­¥æ•°] [è§’åº¦]  - å¤šæ­¥æ‰«ææ‰¾äºº")
    print("  track [é€‰é¡¹] [æ­¥æ•°] [è§’åº¦]  - å®æ—¶è·Ÿè¸ªæ¨¡å¼")
    print("  smart-shot [é€‰é¡¹]           - è·Ÿè¸ª+å³æ‰‹æŠ¬èµ·è‡ªåŠ¨æŠ“æ‹å‘é€")
    print("  shot [æ­¥æ•°] [è§’åº¦]          - æ‹ç…§å¹¶å‘é€")
    print("  calibrate                   - æ ¡å‡†äº‘å°è½¬é€Ÿ")
    print("\né€‰é¡¹:")
    print("  -h, --help                  - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
    print("  -g, --gpu                   - ä½¿ç”¨ GPU ç¡¬è§£")
    print("  -quick, --quick             - é«˜æ€§èƒ½æ¨¡å¼ï¼ˆæ›´çµæ•ï¼Œæ›´è€—ç”µï¼‰")
    print("  --speed <åº¦/ç§’>             - æŒ‡å®šè½¬é€Ÿ")
    print("\nç¤ºä¾‹:")
    print("  python3 -m camera_ng human          # é»˜è®¤æ‰«æ")
    print("  python3 -m camera_ng track -g       # GPU å®æ—¶è·Ÿè¸ª")
    print("  python3 -m camera_ng smart-shot -g -quick  # é«˜çµæ•æ‰‹åŠ¿æŠ“æ‹")
    print("  python3 -m camera_ng shot 8 180     # æ‹ç…§æ¨¡å¼")


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    if len(sys.argv) < 2 or sys.argv[1] in ('-h', '--help'):
        show_help()
        sys.exit(0 if sys.argv[1] in ('-h', '--help') else 1)

    cmd = sys.argv[1]
    args = sys.argv[2:]

    # è§£æ GPU é€‰é¡¹
    use_gpu = False
    if "-g" in args or "--gpu" in args:
        use_gpu = True
        args = [a for a in args if a not in ["-g", "--gpu"]]

    # è§£æé«˜æ€§èƒ½é€‰é¡¹
    quick_mode = False
    if "-quick" in args or "--quick" in args:
        quick_mode = True
        args = [a for a in args if a not in ["-quick", "--quick"]]

    # è§£æè½¬é€Ÿé€‰é¡¹
    global ROTATION_SPEED
    if "--speed" in args:
        speed_idx = args.index("--speed")
        if speed_idx + 1 < len(args):
            ROTATION_SPEED = float(args[speed_idx + 1])
            print(f"âš™ï¸  ä½¿ç”¨æŒ‡å®šè½¬é€Ÿ: {ROTATION_SPEED}Â°/s")
            args = args[:speed_idx] + args[speed_idx + 2:]

    num_steps = int(args[0]) if len(args) > 0 else DEFAULT_NUM_STEPS
    total_angle = float(args[1]) if len(args) > 1 else DEFAULT_TOTAL_ANGLE

    lock = check_single_instance()

    cam = None
    try:
        if cmd == "human":
            cam = SmartCamera()
            result = cam.human(num_steps=num_steps, total_angle=total_angle, use_gpu=use_gpu)
            print(f"\n{'='*60}")
            print(f"æ‰«æç»“æœ: {'æ‰¾åˆ°äºº' if result else 'æœªæ‰¾åˆ°äºº'}")
            print(f"{'='*60}")
            cam.stop()
            
        elif cmd == "shot":
            cam = SmartCamera()
            result = cam.human(num_steps=num_steps, total_angle=total_angle, 
                             use_gpu=use_gpu, center_and_wait=True)
            if result:
                capture_and_send_current_view(cam.camera, "Albertï¼Œæˆ‘æŠ“æ‹åˆ°ä½ å•¦ï¼ğŸ“¸ğŸ’•")
                    
            print(f"\n{'='*60}")
            print(f"æ‹ç…§ç»“æœ: {'æˆåŠŸ' if result else 'æœªæ‰¾åˆ°äºº'}")
            print(f"{'='*60}")
            cam.stop()
            
        elif cmd == "track":
            track_human_realtime(
                num_steps=num_steps,
                total_angle=total_angle,
                use_gpu=use_gpu,
                quick_mode=quick_mode,
            )

        elif cmd == "smart-shot":
            track_human_realtime(
                num_steps=num_steps,
                total_angle=total_angle,
                use_gpu=use_gpu,
                smart_shot=True,
                quick_mode=quick_mode,
            )
            
        elif cmd == "calibrate":
            subprocess.run(["python3", "/home/albert/clawd/scripts/calibrate_speed.py"])
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {cmd}")
            print("æ”¯æŒå‘½ä»¤: human, shot, track, smart-shot, calibrate")
            sys.exit(1)
    finally:
        lock.release()


if __name__ == "__main__":
    main()
