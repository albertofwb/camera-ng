#!/usr/bin/env python3
"""
Mooer Camera NG - ä¸»å…¥å£
é‡æ„åçš„æ¨¡å—åŒ–æ™ºèƒ½è§†è§’æ§åˆ¶ç³»ç»Ÿ
"""

import faulthandler
import fcntl
import os
import subprocess
import sys
import time
from collections import deque

# å¯ç”¨ faulthandlerï¼Œåœ¨å´©æºƒæ—¶æ‰“å° Python å †æ ˆ
faulthandler.enable()

from camera_ng import (
    DEFAULT_NUM_STEPS, DEFAULT_TOTAL_ANGLE,
    ROTATION_SPEED, TRACK_CHECK_INTERVAL, DETECTION_INTERVAL,
    TRACKER_MAX_AGE, TRACKER_MIN_HITS,
    CAPTURE_WIDTH, CAPTURE_HEIGHT, LOCK_FILE,
    CAMERA_RTSP, DEVICE_SERIAL, ACCESS_TOKEN,
    CameraController, VisionAnalyzer, HandRaiseDetector, XiaoxiaoTTS,
    PersonTracker, TrackingMemory
)
from camera_ng.handlers import (
    HandGestureHandler, HandGesture,
    RecordingManager, SmartShotWorker,
)

TELEGRAM_TARGET = "1115213761"


def run_openclaw_send(cmd: list[str], retries: int = 2, timeout_sec: int = 20) -> bool:
    """å‘é€æ¶ˆæ¯ï¼ˆå«è¶…æ—¶ä¸é‡è¯•ï¼‰ï¼Œé¿å…åå°ä»»åŠ¡é•¿æœŸå ç”¨"""
    for attempt in range(1, retries + 1):
        try:
            subprocess.run(cmd, check=True, timeout=timeout_sec)
            return True
        except Exception as e:
            if attempt == retries:
                print(f"âŒ OpenClaw å‘é€å¤±è´¥: {e}")
                return False
            print(f"âš ï¸ OpenClaw å‘é€å¤±è´¥ï¼Œé‡è¯• {attempt}/{retries - 1}: {e}")
            time.sleep(0.6)
    return False


def validate_config():
    """éªŒè¯å¿…è¦é…ç½®æ˜¯å¦å­˜åœ¨"""
    errors = []
    
    if CAMERA_RTSP is None:
        errors.append("  - camera.rtsp_url: RTSP æµåœ°å€æœªé…ç½®")
    if DEVICE_SERIAL is None:
        errors.append("  - camera.device_serial: è®¾å¤‡åºåˆ—å·æœªé…ç½®")
    if ACCESS_TOKEN is None:
        errors.append("  - camera.access_token: è¤çŸ³äº‘ AccessToken æœªé…ç½®")
    
    if errors:
        print("\n" + "=" * 60)
        print("âŒ é…ç½®é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„æ•æ„Ÿä¿¡æ¯")
        print("=" * 60)
        print("\nè¯·åœ¨ä»¥ä¸‹ä½ç½®ä¹‹ä¸€åˆ›å»º camera-config.yaml æ–‡ä»¶ï¼š")
        print("  1. ./memory/camera-config.yaml")
        print("  2. ~/.openclaw/workspace/memory/camera-config.yaml")
        print("  3. ~/clawd/memory/camera-config.yaml")
        print("  4. ~/.config/mooer-camera/config.yaml")
        print("\nç¼ºå¤±çš„é…ç½®é¡¹ï¼š")
        for err in errors:
            print(err)
        print("\ncamera-config.yaml ç¤ºä¾‹æ ¼å¼ï¼š")
        print("""
camera:
  rtsp_url: "rtsp://admin:PASSWORD@IP:554/h264/ch1/main/av_stream"
  device_serial: "YOUR_DEVICE_SERIAL"
  access_token: "YOUR_ACCESS_TOKEN"
  capture:
    seek_time: "00:00:00.5"
    resolution:
      width: 640
      height: 360
    quality: 2
  ptz:
    rotation_speed: 28
""")
        print("=" * 60)
        sys.exit(1)


class SmartCamera:
    """æ™ºèƒ½æ‘„åƒå¤´ç³»ç»Ÿï¼ˆhuman ä¸“ç”¨ï¼‰"""

    def __init__(self):
        self.camera = CameraController()
        self.vision = VisionAnalyzer()

    def human(self, num_steps: int = DEFAULT_NUM_STEPS,
              total_angle: float = DEFAULT_TOTAL_ANGLE,
              use_gpu: bool = False, smart: bool = True,
              fast: bool = False, keep_stream: bool = True,
              center_and_wait: bool = False) -> bool:
        """æ‰§è¡Œ human æ‰«æï¼Œæ‰¾åˆ°äººè¿”å›True"""
        if not self.camera.stream_active:
            print("ğŸ¥ å¯åŠ¨è§†é¢‘æµ...")
            if not self.camera.start_stream(use_gpu=use_gpu):
                print("âŒ æ— æ³•å¯åŠ¨è§†é¢‘æµ")
                return False
            time.sleep(0.5)
        else:
            print("ğŸ”„ å¤ç”¨å·²æœ‰è§†é¢‘æµ...")

        try:
            self.camera.center_and_wait_mode = center_and_wait
            
            if smart:
                result = self.camera.human_steps_smart(self.vision)
            elif fast:
                result = self.camera.human_steps_fast(self.vision)
            else:
                result = self.camera.human_steps(
                    self.vision, num_steps=num_steps, total_angle=total_angle
                )
        finally:
            if not keep_stream:
                self.camera.stop_stream()

        return result
    
    def stop(self):
        """å®Œå…¨åœæ­¢å¹¶æ¸…ç†èµ„æº"""
        self.camera.stop_stream()
        print("âœ… æ™ºèƒ½æ‘„åƒå¤´ç³»ç»Ÿå·²åœæ­¢")
    
    def human_smart_only(self) -> bool:
        """æ™ºèƒ½æ‰«æï¼ˆå¤ç”¨å·²æœ‰æµï¼‰"""
        return self.human(smart=True)


class SingleInstanceLock:
    """å•å®ä¾‹æ–‡ä»¶é”"""

    def __init__(self, lock_file: str = LOCK_FILE):
        self.lock_file = lock_file
        self.fd = None

    def acquire(self) -> bool:
        """è·å–æ–‡ä»¶é”"""
        try:
            self.fd = open(self.lock_file, "w")
            fcntl.flock(self.fd.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
            self.fd.write(str(os.getpid()))
            self.fd.flush()
            return True
        except (IOError, OSError):
            if self.fd:
                self.fd.close()
                self.fd = None
            return False

    def release(self):
        """é‡Šæ”¾æ–‡ä»¶é”"""
        if self.fd:
            try:
                fcntl.flock(self.fd.fileno(), fcntl.LOCK_UN)
                self.fd.close()
                if os.path.exists(self.lock_file):
                    os.remove(self.lock_file)
            except (IOError, OSError):
                pass
            finally:
                self.fd = None

    def __enter__(self):
        if not self.acquire():
            raise RuntimeError("å¦ä¸€ä¸ªå®ä¾‹æ­£åœ¨è¿è¡Œ")
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        self.release()


def capture_and_send_current_view(
    camera: CameraController,
    message: str,
    send_to_tg: bool = False,
) -> bool:
    """åŸºäºå½“å‰ç”»é¢æŠ“æ‹ï¼›å¯é€‰å‘é€ Telegram"""
    output_dir = os.path.expanduser("~/Desktop/capture/pictures")
    os.makedirs(output_dir, exist_ok=True)
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    ms = int((time.time() % 1) * 1000)
    img_path = os.path.join(output_dir, f"{timestamp}_{ms:03d}.jpg")

    camera.capture(output_path=img_path, full_quality=True)
    print(f"ğŸ“¸ å·²ä¿å­˜é«˜è´¨é‡æŠ“æ‹: {img_path}")

    if not send_to_tg:
        print("ğŸ“­ Telegram å‘é€å·²å…³é—­ï¼ˆä½¿ç”¨ --tg å¼€å¯ï¼‰")
        return True

    try:
        send_cmd = [
            "openclaw", "message", "send",
            "--channel", "telegram",
            "--target", TELEGRAM_TARGET,
            "--media", img_path,
            "--message", message
        ]
        print("ğŸ“¤ æ­£åœ¨é€šè¿‡ OpenClaw å‘é€ç…§ç‰‡...")
        ok = run_openclaw_send(send_cmd)
        if ok:
            print("âœ… ç…§ç‰‡å‘é€æˆåŠŸï¼")
        return ok
    except Exception as e:
        print(f"âŒ ç…§ç‰‡å‘é€å¤±è´¥: {e}")
        return False


def send_greeting_voice(tts: XiaoxiaoTTS, message: str, send_to_tg: bool = False) -> bool:
    """å‘é€ä¸­æ–‡é—®å€™è¯­éŸ³ï¼ˆå¯é€‰ Telegramï¼‰"""
    try:
        if tts.playback(message):
            print("ğŸ”ˆ å·²åœ¨æœ¬æœºæ’­æ”¾é—®å€™è¯­éŸ³")
        else:
            print("âš ï¸ æœ¬æœºè¯­éŸ³æ’­æ”¾å¤±è´¥ï¼ˆå·²ç»§ç»­å‘é€ Telegram è¯­éŸ³ï¼‰")

        if not send_to_tg:
            print("ğŸ“­ Telegram è¯­éŸ³å‘é€å·²å…³é—­ï¼ˆä½¿ç”¨ --tg å¼€å¯ï¼‰")
            return True

        voice_path = tts.synthesize(message)
        send_cmd = [
            "openclaw", "message", "send",
            "--channel", "telegram",
            "--target", TELEGRAM_TARGET,
            "--media", voice_path,
            "--message", "å³æ‰‹æ‰‹åŠ¿è¯­éŸ³é—®å€™",
        ]
        print("ğŸ”Š æ­£åœ¨å‘é€é—®å€™è¯­éŸ³...")
        ok = run_openclaw_send(send_cmd)
        if ok:
            print("âœ… è¯­éŸ³å‘é€æˆåŠŸï¼")
        return ok
    except Exception as e:
        print(f"âŒ è¯­éŸ³å‘é€å¤±è´¥: {e}")
        return False


# å·²è¿ç§»åˆ° handlers.py


# å·²è¿ç§»åˆ° handlers.py


def handle_gesture_event(
    event,
    recording_mgr: RecordingManager | None,
    smart_shot_worker: SmartShotWorker,
    current_time: float,
) -> bool:
    """å¤„ç†æ‰‹åŠ¿äº‹ä»¶ï¼Œè¿”å›æ˜¯å¦æˆåŠŸå¤„ç†"""
    from camera_ng.handlers import HandGesture

    if event.gesture == HandGesture.LEFT_HAND:
        # å·¦æ‰‹æ§åˆ¶å½•åƒå¼€å…³
        if recording_mgr is not None:
            recording_mgr.toggle(current_time)
            return True
        return False

    elif event.gesture == HandGesture.RIGHT_HAND:
        # å³æ‰‹è§¦å‘ Smart-Shot
        print("\n   ğŸ™‹ æ£€æµ‹åˆ°å³æ‰‹æŠ¬èµ·ï¼Œè§¦å‘ Smart-Shot...")
        return smart_shot_worker.submit("å³æ‰‹", event.reason)

    return False


def check_single_instance():
    """æ£€æŸ¥æ˜¯å¦å•å®ä¾‹è¿è¡Œ"""
    lock = SingleInstanceLock()
    if not lock.acquire():
        print("âŒ é”™è¯¯ï¼šå¦ä¸€ä¸ª mooer_camera å®ä¾‹æ­£åœ¨è¿è¡Œ")
        print(f"   é”æ–‡ä»¶: {LOCK_FILE}")
        print("   è¯·å…ˆåœæ­¢å…¶ä»–å®ä¾‹å†è¿è¡Œ")
        sys.exit(1)
    return lock


def track_human_realtime(num_steps: int = DEFAULT_NUM_STEPS,
                         total_angle: float = 360,
                         detection_interval: int = DETECTION_INTERVAL,
                         use_gpu: bool = False,
                         smart_shot: bool = False,
                         quick_mode: bool = False,
                         send_to_tg: bool = False,
                         enable_miss: bool = False) -> None:
    """å®æ—¶ç›®æ ‡è·Ÿè¸ªæ¨¡å¼ - ä½¿ç”¨é‡æ„åçš„å¤„ç†å™¨"""
    cam = SmartCamera()
    effective_detection_interval = 1 if quick_mode else detection_interval
    tracker = PersonTracker(
        yolo_model="yolov8n",
        confidence=0.5,
        detection_interval=effective_detection_interval
    )

    # çŠ¶æ€ç®¡ç†
    cycle_count = 0
    person_found = False
    analyzing = False
    lost_count = 0
    LOST_THRESHOLD = 5

    fps_history = deque(maxlen=30)
    last_time = time.time()
    offset_x_history = deque(maxlen=5)
    offset_y_history = deque(maxlen=5)
    recenter_candidate_count = 0
    last_recenter_time = 0.0
    post_found_settle_until = 0.0

    # äº‘å°æ§åˆ¶å‚æ•°
    RECENTER_CONFIRM_FRAMES = 3
    RECENTER_COOLDOWN = 1.2
    BASE_RECENTER_X_THRESHOLD = 0.5
    BASE_RECENTER_Y_THRESHOLD = 0.6
    POST_FOUND_SETTLE_SEC = 0.3

    status_voice_last_ts: dict[str, float] = {}

    def broadcast_status(text: str, min_interval_sec: float = 0.0) -> None:
        if tts is None or not tts.is_available():
            return
        now = time.time()
        last_ts = status_voice_last_ts.get(text, 0.0)
        if min_interval_sec > 0 and (now - last_ts) < min_interval_sec:
            return
        status_voice_last_ts[text] = now
        if tts.playback(text):
            print(f"ğŸ”ˆ å·²æ’­æŠ¥: {text}")

    # Smart-Shot ç»„ä»¶
    tts = XiaoxiaoTTS()
    hand_detector = HandRaiseDetector() if smart_shot else None
    gesture_handler = (
        HandGestureHandler(
            detector=hand_detector,
            confirm_frames=1 if quick_mode else 2,
            release_frames=2 if quick_mode else 3,
            cooldown_sec=0.6 if quick_mode else 1.0,
            log_interval_sec=0.5 if quick_mode else 1.0,
            detect_interval_sec=0.12 if quick_mode else 0.25,
        )
        if smart_shot and hand_detector else None
    )
    recording_mgr = (
        RecordingManager(
            rtsp_url=CAMERA_RTSP,
            tts=tts,
            toggle_cooldown_sec=1.5,
            auto_start_on_person_found=False,
        )
        if smart_shot else None
    )
    def smart_shot_task_callback(camera, hand_text, hand_reason, tts_instance):
        """Smart-Shot ä»»åŠ¡å›è°ƒ"""
        capture_and_send_current_view(
            camera,
            f"Albertï¼Œæˆ‘æ£€æµ‹åˆ°ä½ æŠ¬{hand_text}ï¼Œå·²ä¸ºä½ æŠ“æ‹ï¼ğŸ“¸",
            send_to_tg=send_to_tg,
        )

    smart_shot_worker = (
        SmartShotWorker(
            camera=cam.camera,
            tts=tts,
            telegram_target=TELEGRAM_TARGET,
            max_queue_size=3,
            task_callback=smart_shot_task_callback,
        )
        if smart_shot else None
    )

    if smart_shot and smart_shot_worker:
        smart_shot_worker.start()

    frame_sleep = 0.005 if quick_mode else 0.03
    recenter_pause = 0.2 if quick_mode else 0.5

    print("\n" + "=" * 60)
    print("ğŸ” å¯åŠ¨å®æ—¶ç›®æ ‡è·Ÿè¸ªæ¨¡å¼ (Real-time + SORT)")
    print("=" * 60)
    print(f"é…ç½®: {num_steps}æ­¥/{total_angle}Â°")
    print(f"YOLOæ£€æµ‹é—´éš”: æ¯{effective_detection_interval}å¸§")
    print(f"è·Ÿè¸ªå™¨: SORT (max_age={TRACKER_MAX_AGE}, min_hits={TRACKER_MIN_HITS})")
    print(f"è§†é¢‘è§£ç : {'GPU (CUDA)' if use_gpu else 'CPU'}")
    if smart_shot:
        print("ğŸ“¸ Smart-Shot: å³æ‰‹å¼‚æ­¥ä¿å­˜é«˜è´¨é‡æŠ“æ‹ï¼Œå·¦æ‰‹æŠ¬èµ·å¼€å§‹/åœæ­¢å½•åƒ")
        print("ğŸ–¼ï¸ æŠ“æ‹ç›®å½•: ~/Desktop/capture/pictures")
        print(f"ğŸ“¨ Telegram å‘é€: {'å¼€å¯' if send_to_tg else 'å…³é—­ï¼ˆé»˜è®¤ï¼‰'}")
        if hand_detector is None or hand_detector.model is None:
            print("âš ï¸ Smart-Shot pose æ¨¡å‹ä¸å¯ç”¨ï¼ŒæŠ¬æ‰‹æ£€æµ‹ä¸ä¼šè§¦å‘")
        if tts is None or not tts.is_available():
            print("âš ï¸ æ™“æ™“ TTS ä¸å¯ç”¨ï¼Œå³æ‰‹æŠ¬èµ·åä¸ä¼šå‘é€è¯­éŸ³")
        print("ğŸ“¬ Smart-Shot é˜Ÿåˆ—ç­–ç•¥: drop_oldestï¼ˆé˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§ä»»åŠ¡ï¼‰")
        print(f"ğŸ™‹ æ‰‹åŠ¿æ£€æµ‹é¢‘ç‡: æ¯ {0.12 if quick_mode else 0.25:.2f}s ä¸€æ¬¡ï¼ˆé™ä½è·Ÿè¸ªå¡é¡¿ï¼‰")
        print("ğŸ¬ å½•åƒç­–ç•¥: ä»…å·¦æ‰‹æŠ¬èµ·å¼€å§‹ï¼Œä¸¢å¤±ç›®æ ‡æ—¶è‡ªåŠ¨åœæ­¢")
    print(f"ğŸ”” ç›®æ ‡ä¸¢å¤±æ’­æŠ¥: {'å¼€å¯' if enable_miss else 'å…³é—­ï¼ˆä½¿ç”¨ --enable-miss å¼€å¯ï¼‰'}")
    if quick_mode:
        print("âš¡ Quick æ¨¡å¼: é«˜é¢‘æ£€æµ‹ + æ›´ä½å†·é™æ—¶é—´")
    print("æŒ‰ Ctrl+C åœæ­¢è¿½è¸ª")
    print("=" * 60 + "\n")

    if not cam.camera.start_stream(use_gpu=use_gpu):
        print("âŒ æ— æ³•å¯åŠ¨è§†é¢‘æµ")
        return

    try:
        while True:
            cycle_count += 1
            current_time = time.time()

            fps_history.append(1.0 / (current_time - last_time + 0.001))
            avg_fps = sum(fps_history) / len(fps_history)
            last_time = current_time

            if not person_found:
                # æ‰«ææ‰¾äºº
                print(f"\n{'=' * 60}")
                print(f"ğŸ”„ ç¬¬ {cycle_count} è½® | æ‰§è¡Œæ™ºèƒ½æ‰«æ...")
                print(f"{'=' * 60}")

                person_found = cam.human_smart_only()

                if person_found:
                    print("âœ… æ‰¾åˆ°ç›®æ ‡ï¼")
                    broadcast_status("ç›®æ ‡æ•è·", min_interval_sec=0.8)
                    cam.camera.tracking_memory.reset()

                    if not cam.camera.stream_active:
                        if not cam.camera.start_stream():
                            return
                        time.sleep(0.5)

                    # åˆå§‹åŒ–è·Ÿè¸ª
                    init_attempts = 0
                    max_init_attempts = 10

                    while init_attempts < max_init_attempts:
                        frame = cam.camera.get_frame()
                        if frame is not None:
                            tracks = tracker.update(frame, force_detect=True)
                            if tracks:
                                analyzing = True
                                lost_count = 0
                                break
                        init_attempts += 1
                        time.sleep(0.1)
                    else:
                        person_found = False
                        continue

                    analyzing = True
                    lost_count = 0
                    post_found_settle_until = time.time() + POST_FOUND_SETTLE_SEC
                    print(f"â¸ï¸ æ‰¾åˆ°ç›®æ ‡åç¨³å®š {POST_FOUND_SETTLE_SEC:.1f}sï¼Œå†æ‰§è¡Œç§»åŠ¨")

                    # æ‰¾åˆ°ç›®æ ‡ï¼šæŒ‰é…ç½®å†³å®šæ˜¯å¦è‡ªåŠ¨å¼€å§‹å½•åƒ
                    if recording_mgr:
                        recording_mgr.on_person_found()
                else:
                    print("æœªæ‰¾åˆ°ï¼Œç»§ç»­æ‰«æ...")
                    if not cam.camera.start_stream():
                        cam.camera.start_stream()
                    time.sleep(0.5)

            elif analyzing:
                # å®æ—¶è·Ÿè¸ªæ¨¡å¼
                frame = cam.camera.get_frame()
                if frame is None:
                    time.sleep(0.001)
                    continue

                tracks = tracker.update(frame)
                main_person = tracker.get_main_person()

                if main_person:
                    # è®¡ç®—åç§»å¹¶å¹³æ»‘
                    cx = (main_person.bbox[0] + main_person.bbox[2]) / 2
                    cy = (main_person.bbox[1] + main_person.bbox[3]) / 2
                    offset_x = (cx - CAPTURE_WIDTH/2) / (CAPTURE_WIDTH/2)
                    offset_y = (cy - CAPTURE_HEIGHT/2) / (CAPTURE_HEIGHT/2)

                    offset_x_history.append(offset_x)
                    offset_y_history.append(offset_y)
                    smoothed_offset_x = sum(offset_x_history) / len(offset_x_history)
                    smoothed_offset_y = sum(offset_y_history) / len(offset_y_history)

                    # æ›´æ–°è¿åŠ¨è®°å¿†
                    current_angle = cam.camera.tracking_memory.last_angle
                    if smoothed_offset_x < -0.3:
                        current_angle = (current_angle - 20) % 360
                    elif smoothed_offset_x > 0.3:
                        current_angle = (current_angle + 20) % 360
                    cam.camera.tracking_memory.update(current_angle)

                    # å±…ä¸­é€»è¾‘
                    person_width_ratio = (main_person.bbox[2] - main_person.bbox[0]) / CAPTURE_WIDTH
                    dynamic_x_threshold = BASE_RECENTER_X_THRESHOLD + min(0.25, person_width_ratio * 0.35)
                    need_recenter = (
                        abs(smoothed_offset_x) > dynamic_x_threshold
                        or abs(smoothed_offset_y) > BASE_RECENTER_Y_THRESHOLD
                    )

                    if need_recenter:
                        recenter_candidate_count += 1
                    else:
                        recenter_candidate_count = 0

                    if current_time < post_found_settle_until:
                        recenter_candidate_count = 0

                    if (
                        recenter_candidate_count >= RECENTER_CONFIRM_FRAMES
                        and (current_time - last_recenter_time) >= RECENTER_COOLDOWN
                        and current_time >= post_found_settle_until
                    ):
                        broadcast_status("æ ¡å‡†ä¸­", min_interval_sec=0.5)
                        print(f"\n   ğŸ¯ æŒç»­åç§»è§¦å‘å±…ä¸­: æ°´å¹³{smoothed_offset_x:+.2f}, å‚ç›´{smoothed_offset_y:+.2f}")
                        cam.camera.center_person(smoothed_offset_x, smoothed_offset_y)
                        broadcast_status("æ ¡å‡†å®Œæˆ", min_interval_sec=0.5)
                        recenter_candidate_count = 0
                        last_recenter_time = current_time
                        offset_x_history.clear()
                        offset_y_history.clear()
                        time.sleep(recenter_pause)

                    # æ‰‹åŠ¿æ£€æµ‹
                    if gesture_handler and smart_shot_worker:
                        x1, y1, x2, y2 = [int(v) for v in main_person.bbox]
                        box_w = max(1, x2 - x1)
                        box_h = max(1, y2 - y1)
                        pad_x = int(box_w * 0.2)
                        pad_y = int(box_h * 0.25)

                        gx1 = max(0, x1 - pad_x)
                        gy1 = max(0, y1 - pad_y)
                        gx2 = min(frame.shape[1], x2 + pad_x)
                        gy2 = min(frame.shape[0], y2 + pad_y)

                        gesture_frame = frame[gy1:gy2, gx1:gx2]
                        if gesture_frame.size == 0:
                            gesture_frame = frame

                        event = gesture_handler.update(gesture_frame, current_time)

                        # æ—¥å¿—è¾“å‡º
                        if gesture_handler.should_log(current_time):
                            hand_raised, reason, count = gesture_handler.get_status()
                            status_icon = "âœ“" if hand_raised else "âœ—"
                            confirm_frames = gesture_handler.confirm_frames
                            print(f"\n   ğŸ™‹ æ‰‹åŠ¿: {reason} [å½“å‰å¸§:{status_icon}] | è¿ç»­: {count}/{confirm_frames}")

                        # å¤„ç†è§¦å‘äº‹ä»¶
                        if event:
                            handle_gesture_event(
                                event,
                                recording_mgr,
                                smart_shot_worker,
                                current_time,
                            )

                    lost_count = 0
                else:
                    # ä¸¢å¤±ç›®æ ‡å¤„ç†
                    recenter_candidate_count = 0
                    offset_x_history.clear()
                    offset_y_history.clear()

                    if current_time < post_found_settle_until:
                        time.sleep(frame_sleep)
                        continue

                    lost_count += 1

                    if lost_count >= LOST_THRESHOLD:
                        print(f"\n   âš ï¸ ä¸¢å¤±ç›®æ ‡ï¼Œé‡æ–°æ‰«æ...")
                        if enable_miss:
                            broadcast_status("ç›®æ ‡ä¸¢å¤±", min_interval_sec=1.5)
                        if gesture_handler:
                            gesture_handler.reset()
                        if recording_mgr:
                            recording_mgr.on_person_lost()
                        analyzing = False
                        person_found = False

                time.sleep(frame_sleep)

            else:
                # éåˆ†ææ¨¡å¼ï¼Œä»…æ£€æŸ¥è·Ÿè¸ªçŠ¶æ€
                frame = cam.camera.get_frame()
                if frame is not None:
                    tracks = tracker.update(frame)
                    if tracks:
                        lost_count = 0
                    else:
                        lost_count += 1
                        if lost_count >= LOST_THRESHOLD:
                            if enable_miss:
                                broadcast_status("ç›®æ ‡ä¸¢å¤±", min_interval_sec=1.5)
                            if recording_mgr:
                                recording_mgr.on_person_lost()
                            person_found = False
                time.sleep(TRACK_CHECK_INTERVAL)

    except KeyboardInterrupt:
        print("\n\nåœæ­¢è¿½è¸ª...")
    finally:
        if recording_mgr:
            recording_mgr.cleanup()
        if smart_shot_worker:
            smart_shot_worker.stop()
        cam.camera.stop_stream()
        print("\nè¿½è¸ªå·²åœæ­¢")
        print(f"   å…±æ‰§è¡Œ {cycle_count} è½®")


def show_help():
    """æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"""
    print("Mooer Camera NG - æ™ºèƒ½è§†è§’æ§åˆ¶ç³»ç»Ÿ")
    print("\nç”¨æ³•:")
    print("  python3 -m camera_ng --help")
    print("  python3 -m camera_ng <å‘½ä»¤> [é€‰é¡¹] [æ­¥æ•°] [è§’åº¦]")
    print("  python3 -m camera_ng <å‘½ä»¤> --help")
    print("\nå¯ç”¨å‘½ä»¤:")
    print("  human [é€‰é¡¹] [æ­¥æ•°] [è§’åº¦]  - å¤šæ­¥æ‰«ææ‰¾äºº")
    print("  track [é€‰é¡¹] [æ­¥æ•°] [è§’åº¦]  - å®æ—¶è·Ÿè¸ªæ¨¡å¼")
    print("  smart-shot [é€‰é¡¹]           - è·Ÿè¸ª+å³æ‰‹æŠ“æ‹(å¼‚æ­¥ä¿å­˜)+å·¦æ‰‹å½•åƒ")
    print("  shot [æ­¥æ•°] [è§’åº¦]          - æ‹ç…§å¹¶å‘é€")
    print("  prepare-tts [é€‰é¡¹]          - é¢„ç”Ÿæˆå¸¸ç”¨æœ¬æœºæç¤ºéŸ³åˆ° media/tts")
    print("  calibrate                   - æ ¡å‡†äº‘å°è½¬é€Ÿ")
    print("\né€‰é¡¹:")
    print("  -h, --help                  - æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")
    print("  -g, --gpu                   - ä½¿ç”¨ GPU ç¡¬è§£")
    print("  -quick, --quick             - é«˜æ€§èƒ½æ¨¡å¼ï¼ˆæ›´çµæ•ï¼Œæ›´è€—ç”µï¼‰")
    print("  --tg, --telegram            - å¼€å¯ Telegram å‘é€ï¼ˆé»˜è®¤å…³é—­ï¼‰")
    print("  --enable-miss              - å¼€å¯â€œç›®æ ‡ä¸¢å¤±â€è¯­éŸ³æ’­æŠ¥")
    print("  --overwrite                 - ä»…ç”¨äº prepare-ttsï¼Œè¦†ç›–å·²æœ‰éŸ³é¢‘")
    print("  --speed <åº¦/ç§’>             - æŒ‡å®šè½¬é€Ÿ")
    print("\nç¤ºä¾‹:")
    print("  python3 -m camera_ng human          # é»˜è®¤æ‰«æ")
    print("  python3 -m camera_ng track -g       # GPU å®æ—¶è·Ÿè¸ª")
    print("  python3 -m camera_ng smart-shot -g -quick  # é«˜çµæ•æ‰‹åŠ¿æŠ“æ‹")
    print("  python3 -m camera_ng smart-shot -g --tg     # å¼€å¯ Telegram å‘é€")
    print("  python3 -m camera_ng shot 8 180 --tg         # æ‹ç…§å¹¶å‘é€")
    print("  python3 -m camera_ng prepare-tts             # é¢„ç”Ÿæˆæœ¬åœ°æç¤ºéŸ³")
    print("\nSmart-Shot è¡Œä¸º:")
    print("  - å³æ‰‹æŠ¬èµ·ï¼šå¼‚æ­¥ä¿å­˜é«˜è´¨é‡ç…§ç‰‡åˆ° ~/Desktop/capture/pictures/<timestamp>.jpg")
    print("  - å·¦æ‰‹æŠ¬èµ·ï¼šå¼€å§‹/åœæ­¢å½•åƒ")
    print("  - ç›®æ ‡æ•è·/æ ¡å‡†ä¸­/æ ¡å‡†å®Œæˆï¼šé»˜è®¤æ’­æŠ¥")
    print("  - ç›®æ ‡ä¸¢å¤±ï¼šä»… --enable-miss æ—¶æ’­æŠ¥")
    print("  - Telegram é»˜è®¤å…³é—­ï¼›åŠ  --tg æ‰å‘é€")


def main():
    """ä¸»å…¥å£å‡½æ•°"""
    if len(sys.argv) < 2 or sys.argv[1] in ('-h', '--help'):
        show_help()
        sys.exit(0 if sys.argv[1] in ('-h', '--help') else 1)

    cmd = sys.argv[1]
    args = sys.argv[2:]

    if "-h" in args or "--help" in args:
        show_help()
        sys.exit(0)

    # è§£æ GPU é€‰é¡¹
    use_gpu = False
    if "-g" in args or "--gpu" in args:
        use_gpu = True
        args = [a for a in args if a not in ["-g", "--gpu"]]

    # è§£æé«˜æ€§èƒ½é€‰é¡¹
    quick_mode = False
    if "-quick" in args or "--quick" in args:
        quick_mode = True
        args = [a for a in args if a not in ["-quick", "--quick"]]

    # è§£æ Telegram å‘é€é€‰é¡¹
    send_to_tg = False
    if "--tg" in args or "--telegram" in args:
        send_to_tg = True
        args = [a for a in args if a not in ["--tg", "--telegram"]]

    overwrite_tts = False
    if "--overwrite" in args:
        overwrite_tts = True
        args = [a for a in args if a != "--overwrite"]

    enable_miss = False
    if "--enable-miss" in args:
        enable_miss = True
        args = [a for a in args if a != "--enable-miss"]

    # è§£æè½¬é€Ÿé€‰é¡¹
    global ROTATION_SPEED
    if "--speed" in args:
        speed_idx = args.index("--speed")
        if speed_idx + 1 < len(args):
            ROTATION_SPEED = float(args[speed_idx + 1])
            print(f"âš™ï¸  ä½¿ç”¨æŒ‡å®šè½¬é€Ÿ: {ROTATION_SPEED}Â°/s")
            args = args[:speed_idx] + args[speed_idx + 2:]

    num_steps = int(args[0]) if len(args) > 0 and args[0].lstrip("-").isdigit() else DEFAULT_NUM_STEPS
    total_angle = float(args[1]) if len(args) > 1 and args[1].replace(".", "", 1).lstrip("-").isdigit() else DEFAULT_TOTAL_ANGLE

    config_required_cmds = {"human", "shot", "track", "smart-shot"}
    if cmd in config_required_cmds:
        validate_config()

    lock = check_single_instance() if cmd in config_required_cmds else None

    cam = None
    try:
        if cmd == "human":
            cam = SmartCamera()
            result = cam.human(num_steps=num_steps, total_angle=total_angle, use_gpu=use_gpu)
            print(f"\n{'='*60}")
            print(f"æ‰«æç»“æœ: {'æ‰¾åˆ°äºº' if result else 'æœªæ‰¾åˆ°äºº'}")
            print(f"{'='*60}")
            cam.stop()
            
        elif cmd == "shot":
            cam = SmartCamera()
            result = cam.human(num_steps=num_steps, total_angle=total_angle, 
                             use_gpu=use_gpu, center_and_wait=True)
            if result:
                capture_and_send_current_view(
                    cam.camera,
                    "Albertï¼Œæˆ‘æŠ“æ‹åˆ°ä½ å•¦ï¼ğŸ“¸ğŸ’•",
                    send_to_tg=send_to_tg,
                )
                    
            print(f"\n{'='*60}")
            print(f"æ‹ç…§ç»“æœ: {'æˆåŠŸ' if result else 'æœªæ‰¾åˆ°äºº'}")
            print(f"{'='*60}")
            cam.stop()
            
        elif cmd == "track":
            track_human_realtime(
                num_steps=num_steps,
                total_angle=total_angle,
                use_gpu=use_gpu,
                quick_mode=quick_mode,
                send_to_tg=send_to_tg,
                enable_miss=enable_miss,
            )

        elif cmd == "smart-shot":
            track_human_realtime(
                num_steps=num_steps,
                total_angle=total_angle,
                use_gpu=use_gpu,
                smart_shot=True,
                quick_mode=quick_mode,
                send_to_tg=send_to_tg,
                enable_miss=enable_miss,
            )

        elif cmd == "prepare-tts":
            tts = XiaoxiaoTTS()
            created, skipped = tts.pregenerate_common_prompts(overwrite=overwrite_tts)
            print("\n" + "=" * 60)
            print("ğŸ”Š é¢„ç”Ÿæˆæç¤ºéŸ³å®Œæˆ")
            print(f"   ç›®å½•: {tts.media_dir}")
            print(f"   æ–°ç”Ÿæˆ: {created}")
            print(f"   è·³è¿‡: {skipped}")
            print("=" * 60)
            
        elif cmd == "calibrate":
            subprocess.run(["python3", "/home/albert/clawd/scripts/calibrate_speed.py"])
        else:
            print(f"æœªçŸ¥å‘½ä»¤: {cmd}")
            print("æ”¯æŒå‘½ä»¤: human, shot, track, smart-shot, prepare-tts, calibrate")
            sys.exit(1)
    finally:
        if lock is not None:
            lock.release()


if __name__ == "__main__":
    main()
